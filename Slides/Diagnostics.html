<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>16: Diagnostics.</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jaye Seawright" />
    <meta name="date" content="2026-03-02" />
    <script src="Diagnostics_files/header-attrs-2.30/header-attrs.js"></script>
    <script src="Diagnostics_files/kePrint-0.0.1/kePrint.js"></script>
    <link href="Diagnostics_files/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 16: Diagnostics.
]
.subtitle[
## Linear Models
]
.author[
### <large>Jaye Seawright</large>
]
.institute[
### <small>Northwestern Political Science</small>
]
.date[
### March 2, 2026
]

---

class: center, middle


&lt;style type="text/css"&gt;
pre {
  max-height: 400px;
  overflow-y: auto;
}

pre[class] {
  max-height: 200px;
}
&lt;/style&gt;




### **The Problem**
- Regression models make strong assumptions

---
### **The Problem**

- Violations can lead to:
  - **Biased coefficients**
  - **Invalid inferences**
  - **Poor predictions**

---
# Key Assumptions of Regression

### **1. Validity** 
Data fit the concepts and populations in the research question
- Outcome measures the phenomenon
- Model includes relevant predictors
- Generalizes to target cases

---
# Key Assumptions of Regression

### **2. Representativeness**
Sample reflects population (conditional on X)
- Selection on X is OK
- Selection on Y is problematic
- Even with "all" data (e.g., all elections)


---
# Key Assumptions of Regression

### **3. Additivity &amp; Linearity**
Predictors combine linearly
- Some political relationships are nonlinear
- Age × voting, income × participation
- Nonlinear relationships are fine if we can include them as linear terms

---
# Key Assumptions of Regression


### **4. Independence of Errors**
No autocorrelation (time/space)

---
# Key Assumptions of Regression


### **5. Equal Variance (Homoscedasticity)**
Constant prediction uncertainty

---
# Key Assumptions of Regression


### **6. Normality of Errors**
Least important for estimation

---
# Representativeness: Selection Bias

### **The Core Issue**
Selection on Y biases coefficients

---


``` r
set.seed(123)
n &lt;- 1000
pop_data &lt;- data.frame(
  ideology = rnorm(n, 0, 1),
  income = rnorm(n, 50000, 15000)
)

# Make voting probability depend on ideology and income
prob_vote &lt;- plogis(0.5 + 0.3*pop_data$ideology + pop_data$income/100000)
pop_data$vote &lt;- rbinom(n, 1, prob_vote)

# Selection: only voters in sample
sample_data &lt;- pop_data %&gt;% 
  filter(vote == 1) %&gt;% 
  slice_sample(n = 200)

cat("Population correlation (ideology, income):", 
    round(cor(pop_data$ideology, pop_data$income), 3), "\n")
```

```
## Population correlation (ideology, income): 0.086
```

``` r
cat("Sample correlation (ideology, income):", 
    round(cor(sample_data$ideology, sample_data$income), 3))
```

```
## Sample correlation (ideology, income): 0.075
```



---
# Representativeness: Selection Bias

### **Political Science Examples**

1. **Survey non-response**
   - Political interest → participation → survey response
2. **Media coverage**
   - Extreme cases get coverage → biased perception

---
# Representativeness: Selection Bias

### **Political Science Examples**

3\. **Historical records**
  - Surviving documents ≠ all documents
  
4\. **Elite interviews**
  - Accessible elites ≠ all elites

---
# Representativeness: Selection Bias

### **Solutions**
- Weighting
- Selection models
- Multiple data sources

---
# Additivity &amp; Linearity: Nonlinear Reality

| | | |
|:---:|:---:|:---:|
| ![Image1](Images/ageturnout.png) | ![Image2](Images/incometurnout.png) | ![Image3](Images/gendertime.jpg) |

Always check for nonlinearity. Consider transformations or flexible models.

---
# Diagnostic Tool 1: Residual Plots

### **What to Plot**

``` r
la_electoral &lt;- read.csv("https://raw.githubusercontent.com/jnseawright/ps405/refs/heads/main/Data/laelectoral.csv")

# After fitting model
fit &lt;- lm(voteswoninc ~ gdpgrowthlag + votesinclag, data = la_electoral)
```

---



``` r
# Base R
plot(fit, which = 1)  # Residuals vs Fitted
```

![](Diagnostics_files/figure-html/residual-code-2-1.png)&lt;!-- --&gt;

---


``` r
la_electoral$countryname[c(70,72,62)]
```

```
## [1] "Peru"      "Peru"      "Guatemala"
```

``` r
la_electoral$electiondate[c(70,72,62)]
```

```
## [1] "06/04/06" "06/05/11" "12/28/03"
```

---


``` r
# ggplot2
library(ggplot2)
la_electoral$residuals &lt;- resid(fit)
la_electoral$fitted &lt;- fitted(fit)

residfit.plot &lt;- ggplot(la_electoral, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE)
```

---

&lt;img src="Diagnostics_files/figure-html/unnamed-chunk-2-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

### **What to Look For**

![](Diagnostics_files/figure-html/residual-patterns-1.png)&lt;!-- --&gt;

---
# Diagnostic Tool 2: Posterior Predictive Checks

### **The Idea**
1. Fit your model
2. Simulate new data from it
3. Compare simulated data to real data
4. If they look different → model problems

---
### **Bayesian Implementation**

``` r
# Using rstanarm
library(rstanarm)
fit_bayes &lt;- stan_glm(voteswoninc ~ gdpgrowthlag + votesinclag, 
                      data = la_electoral,
                      family = gaussian(),
                      seed = 123)
```

```
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 6.6e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.66 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.063 seconds (Warm-up)
## Chain 1:                0.055 seconds (Sampling)
## Chain 1:                0.118 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.4e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.063 seconds (Warm-up)
## Chain 2:                0.056 seconds (Sampling)
## Chain 2:                0.119 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.6e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.062 seconds (Warm-up)
## Chain 3:                0.048 seconds (Sampling)
## Chain 3:                0.11 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.3e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.048 seconds (Warm-up)
## Chain 4:                0.053 seconds (Sampling)
## Chain 4:                0.101 seconds (Total)
## Chain 4:
```

---


``` r
# Generate posterior predictions
y_rep &lt;- posterior_predict(fit_bayes)

# Compare distributions
bayesplot::ppc_dens_overlay(la_electoral$voteswoninc, y_rep)
```

![](Diagnostics_files/figure-html/ppc-code-2-1.png)&lt;!-- --&gt;

---
# Diagnostic Tool 3: Cross-Validation

### **Why CV?**
- In-sample fit ≠ out-of-sample performance
- Prevents overfitting
- Estimates predictive accuracy

---

### **Types of CV**
1. **Leave-One-Out (LOO)**
   - Each observation left out once
   - Computationally efficient approximation

2. **K-Fold** (e.g., 10-fold)
   - Random partitions
   - More stable with outliers

3. **Time-Series CV**
   - Train on past, test on future
   - Critical for political forecasting

---
### **Implementation in R**


``` r
library(rstanarm)
library(loo)

# Fit Bayesian model
fit &lt;- stan_glm(vdem_libdem ~ I(log(wdi_gdpcappppcon2017)) + wdi_gerp,
                data = qogts)
```

```
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.6e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.048 seconds (Warm-up)
## Chain 1:                0.369 seconds (Sampling)
## Chain 1:                0.417 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.1e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.045 seconds (Warm-up)
## Chain 2:                0.397 seconds (Sampling)
## Chain 2:                0.442 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 3.1e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.059 seconds (Warm-up)
## Chain 3:                0.35 seconds (Sampling)
## Chain 3:                0.409 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.2e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.044 seconds (Warm-up)
## Chain 4:                0.345 seconds (Sampling)
## Chain 4:                0.389 seconds (Total)
## Chain 4:
```

``` r
# LOO Cross-Validation
loo_result &lt;- loo(fit)
print(loo_result)
```

```
## 
## Computed from 4000 by 4177 log-likelihood matrix.
## 
##          Estimate   SE
## elpd_loo    431.4 43.4
## p_loo         3.8  0.1
## looic      -862.9 86.8
## ------
## MCSE of elpd_loo is 0.0.
## MCSE and ESS estimates assume independent draws (r_eff=1).
## 
## All Pareto k estimates are good (k &lt; 0.7).
## See help('pareto-k-diagnostic') for details.
```

``` r
# Compare models
fit2 &lt;- stan_glm(vdem_libdem ~ I(log(wdi_gdpcappppcon2017)) + wdi_gerp + as.factor(ht_colonial), data = qogts)
```

```
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 3.5e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.082 seconds (Warm-up)
## Chain 1:                0.393 seconds (Sampling)
## Chain 1:                0.475 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.7e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.081 seconds (Warm-up)
## Chain 2:                0.412 seconds (Sampling)
## Chain 2:                0.493 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.9e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.081 seconds (Warm-up)
## Chain 3:                0.441 seconds (Sampling)
## Chain 3:                0.522 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.4e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.074 seconds (Warm-up)
## Chain 4:                0.395 seconds (Sampling)
## Chain 4:                0.469 seconds (Total)
## Chain 4:
```

``` r
loo2 &lt;- loo(fit2)
loo_compare(loo_result, loo2)
```

```
##      elpd_diff se_diff
## fit2    0.0       0.0 
## fit  -215.9      22.6
```

---

### **Interpretation**
- `elpd_diff`: Difference in expected log predictive density
- Rule of thumb: `elpd_diff &gt; 4` suggests meaningful difference

---
# `\(R^2\)` and Explained Variance

`$$R^2 = 1 - \frac{\sum_{i=1}^{n} e_{i}^2}{\sum_{i=1}^{n} y_{i}^2}$$`
---

### **What `\(R^2\)` Tells Us**
- Proportion of variance "explained"
- Range: 0 (worst) to 1 (best)

---

### **But in social science...**
- **Low `\(R^2\)` is common.**
- Political behavior is noisy
- Measurement error, multiple causation, heterogeneity

---

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Study &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; R2 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Interpretation &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Populism and Referenda (Werner and Jacobs 2022) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.06 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; A bit low, but okay &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Strongmen and Geographic Vote Choice (Hong et al. 2023) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.84 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Double-check the model &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Explaining Populism (Schafer 2022) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.17 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Normal for individual data &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


---

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Study &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; R2 &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Interpretation &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Supreme Court Legitimacy (Gibson 2025) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.24 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Good for individual data &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Subnational Democratic Backsliding (Grumbach 2023) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.68 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; High numbers because aggregate &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### **Bayesian `\(R^2\)`**
- Mathematicaly different because the variance of Bayesian predicted values can sometimes be higher than the variance of the original data

---


``` r
# Bayesian R2 with uncertainty
bayes_R2(fit)  # Returns posterior distribution
```

```
##    [1] 0.3396166 0.3386430 0.3313003 0.3730969 0.3718251 0.3276172 0.3286093
##    [8] 0.3577287 0.3541902 0.3603286 0.3384263 0.3441835 0.3473017 0.3483312
##   [15] 0.3495833 0.3551821 0.3408266 0.3310016 0.3287466 0.3217190 0.3425883
##   [22] 0.3487087 0.3525128 0.3331316 0.3477275 0.3506395 0.3501636 0.3403135
##   [29] 0.3606052 0.3511122 0.3417575 0.3571168 0.3284393 0.3308561 0.3410686
##   [36] 0.3384257 0.3486632 0.3516482 0.3436331 0.3459028 0.3397124 0.3598273
##   [43] 0.3346258 0.3397453 0.3392089 0.3486448 0.3550168 0.3335122 0.3480227
##   [50] 0.3439507 0.3510570 0.3583219 0.3407133 0.3444290 0.3424521 0.3557923
##   [57] 0.3383277 0.3443130 0.3413673 0.3407202 0.3431716 0.3401417 0.3344615
##   [64] 0.3564671 0.3343430 0.3451595 0.3489630 0.3497753 0.3382268 0.3572528
##   [71] 0.3235817 0.3697645 0.3327496 0.3617985 0.3287517 0.3601201 0.3537905
##   [78] 0.3333321 0.3668357 0.3352188 0.3353349 0.3572160 0.3651411 0.3566466
##   [85] 0.3588059 0.3546446 0.3394246 0.3532399 0.3669662 0.3408328 0.3507314
##   [92] 0.3264370 0.3329724 0.3473447 0.3566935 0.3477165 0.3402771 0.3320656
##   [99] 0.3612938 0.3338616 0.3253798 0.3493261 0.3452237 0.3392379 0.3257987
##  [106] 0.3337952 0.3575580 0.3367810 0.3647478 0.3328034 0.3582136 0.3436552
##  [113] 0.3614231 0.3253883 0.3487303 0.3496667 0.3471740 0.3425388 0.3761515
##  [120] 0.3382409 0.3348382 0.3574109 0.3454960 0.3396022 0.3593842 0.3492162
##  [127] 0.3680729 0.3370543 0.3701047 0.3689598 0.3457772 0.3422335 0.3437523
##  [134] 0.3421537 0.3559780 0.3496401 0.3623681 0.3568037 0.3295314 0.3245831
##  [141] 0.3341593 0.3218797 0.3515427 0.3346030 0.3523807 0.3416028 0.3424479
##  [148] 0.3511804 0.3461496 0.3566486 0.3307580 0.3586246 0.3352168 0.3416737
##  [155] 0.3365648 0.3435197 0.3516521 0.3375433 0.3335659 0.3313840 0.3371890
##  [162] 0.3582807 0.3426315 0.3613984 0.3567996 0.3628824 0.3363443 0.3404634
##  [169] 0.3477671 0.3444923 0.3380881 0.3453082 0.3416479 0.3167441 0.3433807
##  [176] 0.3479633 0.3662861 0.3669135 0.3507793 0.3434876 0.3468957 0.3331041
##  [183] 0.3522592 0.3455091 0.3639836 0.3553685 0.3444607 0.3551036 0.3483461
##  [190] 0.3486289 0.3426941 0.3570587 0.3508625 0.3366711 0.3338564 0.3396249
##  [197] 0.3464350 0.3543605 0.3598692 0.3521289 0.3384222 0.3458066 0.3402408
##  [204] 0.3360567 0.3342411 0.3269646 0.3537156 0.3329994 0.3627487 0.3404320
##  [211] 0.3470274 0.3388548 0.3320035 0.3355807 0.3306659 0.3380821 0.3387584
##  [218] 0.3680998 0.3278223 0.3148809 0.3497279 0.3479883 0.3425419 0.3557492
##  [225] 0.3382109 0.3440443 0.3547035 0.3421111 0.3440670 0.3615111 0.3578945
##  [232] 0.3329322 0.3418019 0.3366701 0.3494990 0.3486807 0.3439039 0.3539588
##  [239] 0.3390275 0.3500038 0.3480989 0.3486548 0.3320467 0.3495249 0.3344627
##  [246] 0.3468990 0.3204941 0.3325693 0.3438920 0.3553568 0.3404417 0.3535240
##  [253] 0.3491180 0.3474793 0.3480259 0.3443313 0.3461472 0.3471421 0.3464826
##  [260] 0.3377956 0.3485942 0.3572928 0.3272306 0.3434775 0.3488435 0.3485986
##  [267] 0.3513179 0.3447984 0.3320035 0.3480485 0.3320846 0.3550299 0.3340487
##  [274] 0.3532301 0.3360878 0.3530643 0.3391252 0.3471211 0.3438291 0.3562805
##  [281] 0.3273389 0.3238551 0.3739184 0.3219088 0.3093367 0.3306328 0.3583149
##  [288] 0.3292164 0.3591108 0.3316388 0.3367197 0.3490684 0.3476407 0.3521515
##  [295] 0.3493612 0.3366413 0.3340644 0.3491776 0.3271906 0.3600146 0.3356392
##  [302] 0.3465618 0.3702603 0.3452994 0.3497894 0.3521049 0.3448743 0.3438690
##  [309] 0.3615739 0.3296925 0.3269846 0.3370974 0.3536326 0.3498350 0.3469426
##  [316] 0.3182059 0.3714213 0.3401618 0.3471068 0.3416684 0.3570394 0.3568921
##  [323] 0.3515626 0.3346569 0.3608470 0.3194749 0.3547789 0.3550868 0.3413128
##  [330] 0.3487403 0.3402706 0.3530099 0.3603061 0.3405464 0.3396819 0.3162112
##  [337] 0.3343545 0.3424358 0.3570671 0.3330503 0.3499634 0.3248895 0.3485731
##  [344] 0.3371969 0.3510034 0.3465560 0.3415783 0.3511543 0.3435462 0.3289325
##  [351] 0.3534563 0.3517220 0.3484341 0.3597861 0.3243762 0.3469107 0.3457547
##  [358] 0.3437768 0.3494226 0.3489086 0.3531580 0.3376933 0.3516607 0.3330187
##  [365] 0.3656730 0.3395948 0.3394949 0.3244356 0.3319525 0.3531954 0.3239380
##  [372] 0.3577777 0.3372608 0.3695544 0.3224550 0.3248601 0.3398641 0.3591505
##  [379] 0.3559986 0.3429202 0.3579911 0.3413678 0.3388576 0.3390939 0.3332381
##  [386] 0.3303750 0.3437166 0.3475540 0.3441440 0.3562129 0.3518041 0.3416760
##  [393] 0.3449997 0.3478037 0.3583550 0.3637267 0.3408297 0.3401345 0.3289486
##  [400] 0.3580013 0.3384966 0.3322028 0.3544608 0.3441997 0.3490886 0.3592791
##  [407] 0.3223918 0.3828435 0.3151195 0.3488241 0.3444922 0.3520149 0.3453186
##  [414] 0.3456514 0.3417976 0.3342215 0.3454269 0.3332385 0.3552254 0.3449882
##  [421] 0.3428609 0.3390524 0.3556242 0.3227664 0.3340578 0.3301654 0.3282773
##  [428] 0.3459904 0.3397059 0.3454412 0.3414006 0.3566386 0.3377322 0.3447028
##  [435] 0.3493293 0.3511695 0.3407535 0.3366736 0.3581744 0.3456439 0.3443116
##  [442] 0.3439190 0.3432760 0.3181694 0.3573152 0.3345009 0.3599405 0.3464770
##  [449] 0.3361086 0.3453084 0.3450691 0.3578963 0.3352185 0.3266663 0.3379177
##  [456] 0.3491441 0.3390545 0.3393394 0.3490159 0.3560536 0.3283271 0.3608422
##  [463] 0.3387689 0.3448643 0.3574583 0.3268731 0.3755060 0.3317812 0.3231381
##  [470] 0.3225413 0.3540289 0.3343696 0.3377140 0.3703928 0.3301166 0.3379630
##  [477] 0.3631749 0.3570201 0.3425641 0.3436650 0.3562670 0.3257843 0.3652578
##  [484] 0.3355424 0.3271206 0.3340870 0.3432837 0.3378247 0.3492036 0.3416229
##  [491] 0.3437430 0.3494918 0.3445135 0.3422843 0.3353129 0.3476111 0.3234284
##  [498] 0.3462460 0.3579348 0.3464938 0.3452797 0.3389424 0.3463547 0.3532974
##  [505] 0.3721634 0.3301559 0.3344811 0.3574001 0.3650140 0.3164370 0.3407694
##  [512] 0.3398132 0.3459730 0.3323347 0.3341557 0.3486413 0.3397868 0.3466596
##  [519] 0.3373143 0.3490525 0.3528753 0.3485403 0.3481998 0.3546154 0.3413263
##  [526] 0.3453957 0.3453294 0.3502777 0.3454772 0.3538458 0.3600620 0.3715378
##  [533] 0.3633605 0.3671436 0.3619970 0.3439255 0.3335378 0.3445263 0.3630235
##  [540] 0.3470934 0.3502520 0.3347762 0.3455963 0.3467382 0.3626838 0.3443963
##  [547] 0.3564209 0.3595546 0.3553822 0.3360372 0.3246472 0.3697811 0.3549874
##  [554] 0.3308089 0.3514124 0.3500981 0.3393194 0.3298711 0.3575631 0.3492895
##  [561] 0.3418237 0.3382189 0.3539735 0.3411313 0.3417049 0.3393162 0.3521529
##  [568] 0.3447123 0.3590475 0.3519686 0.3339874 0.3347989 0.3524596 0.3542265
##  [575] 0.3227052 0.3544848 0.3535476 0.3305080 0.3546623 0.3333471 0.3504954
##  [582] 0.3396226 0.3512947 0.3517461 0.3407620 0.3437875 0.3438816 0.3476980
##  [589] 0.3438041 0.3504502 0.3615868 0.3336214 0.3668478 0.3257484 0.3664249
##  [596] 0.3354281 0.3284892 0.3501242 0.3486242 0.3398199 0.3457630 0.3500207
##  [603] 0.3365215 0.3611676 0.3516363 0.3526134 0.3475658 0.3333659 0.3614448
##  [610] 0.3364493 0.3320846 0.3641313 0.3304566 0.3596729 0.3245145 0.3316659
##  [617] 0.3289413 0.3323086 0.3257172 0.3498024 0.3410317 0.3479132 0.3441861
##  [624] 0.3591648 0.3295023 0.3462353 0.3439122 0.3333319 0.3564696 0.3349309
##  [631] 0.3440721 0.3461258 0.3490652 0.3626803 0.3585069 0.3665658 0.3216361
##  [638] 0.3485471 0.3511396 0.3386070 0.3325444 0.3575787 0.3325022 0.3474304
##  [645] 0.3394166 0.3484443 0.3455421 0.3269923 0.3589216 0.3438823 0.3554123
##  [652] 0.3467731 0.3485492 0.3367611 0.3429629 0.3488214 0.3459559 0.3239877
##  [659] 0.3577179 0.3459266 0.3478439 0.3505346 0.3582040 0.3448522 0.3525271
##  [666] 0.3387038 0.3733610 0.3275794 0.3634791 0.3466688 0.3475071 0.3331862
##  [673] 0.3484572 0.3346176 0.3458313 0.3432829 0.3450273 0.3463157 0.3430320
##  [680] 0.3533234 0.3535158 0.3498543 0.3525854 0.3371437 0.3569115 0.3397311
##  [687] 0.3439812 0.3439753 0.3505789 0.3415829 0.3613720 0.3446717 0.3503729
##  [694] 0.3497445 0.3436912 0.3338660 0.3491024 0.3436667 0.3582022 0.3496843
##  [701] 0.3486975 0.3532045 0.3477664 0.3426808 0.3380043 0.3354274 0.3374261
##  [708] 0.3501438 0.3492149 0.3442910 0.3416417 0.3466362 0.3396943 0.3523029
##  [715] 0.3550761 0.3503684 0.3409642 0.3406031 0.3434256 0.3527913 0.3356659
##  [722] 0.3564962 0.3320360 0.3535580 0.3449290 0.3423166 0.3451552 0.3594571
##  [729] 0.3233255 0.3337605 0.3526434 0.3362850 0.3560505 0.3511356 0.3325978
##  [736] 0.3405815 0.3276178 0.3302960 0.3232272 0.3444734 0.3266715 0.3588375
##  [743] 0.3562996 0.3279813 0.3283202 0.3411700 0.3525321 0.3532280 0.3261143
##  [750] 0.3690331 0.3420375 0.3361336 0.3553377 0.3340384 0.3456486 0.3459122
##  [757] 0.3419495 0.3369795 0.3463394 0.3454837 0.3418136 0.3576272 0.3526786
##  [764] 0.3469486 0.3450583 0.3408237 0.3365157 0.3569546 0.3398644 0.3416178
##  [771] 0.3523444 0.3401738 0.3522147 0.3491911 0.3379839 0.3507158 0.3486734
##  [778] 0.3442931 0.3393398 0.3113368 0.3423043 0.3523829 0.3427683 0.3354654
##  [785] 0.3536255 0.3244036 0.3275709 0.3673296 0.3257467 0.3447035 0.3432804
##  [792] 0.3480853 0.3427048 0.3449527 0.3420688 0.3562113 0.3465478 0.3447944
##  [799] 0.3458616 0.3259158 0.3532339 0.3426002 0.3352875 0.3626248 0.3355291
##  [806] 0.3238407 0.3692602 0.3441921 0.3447914 0.3552349 0.3594271 0.3540011
##  [813] 0.3508714 0.3584749 0.3138457 0.3615509 0.3325017 0.3541197 0.3330450
##  [820] 0.3278526 0.3322585 0.3206067 0.3599230 0.3311883 0.3379232 0.3655991
##  [827] 0.3603174 0.3313843 0.3538410 0.3755500 0.3208601 0.3304768 0.3469022
##  [834] 0.3383995 0.3464323 0.3480996 0.3438906 0.3520099 0.3372903 0.3505592
##  [841] 0.3318175 0.3499474 0.3394854 0.3298795 0.3383621 0.3510937 0.3687562
##  [848] 0.3338339 0.3399745 0.3506955 0.3298246 0.3548957 0.3384650 0.3358328
##  [855] 0.3457474 0.3439088 0.3410475 0.3467660 0.3522343 0.3553633 0.3585786
##  [862] 0.3429502 0.3422898 0.3415911 0.3449751 0.3443331 0.3462913 0.3312227
##  [869] 0.3426807 0.3373724 0.3412472 0.3437342 0.3483990 0.3426824 0.3337262
##  [876] 0.3528509 0.3163043 0.3596927 0.3591600 0.3332842 0.3510355 0.3364914
##  [883] 0.3438008 0.3464091 0.3400984 0.3491039 0.3431404 0.3319600 0.3687561
##  [890] 0.3211653 0.3506175 0.3332766 0.3506789 0.3414310 0.3405113 0.3590956
##  [897] 0.3591402 0.3469231 0.3624527 0.3510065 0.3683541 0.3547443 0.3558467
##  [904] 0.3548369 0.3403492 0.3436428 0.3362357 0.3676039 0.3316169 0.3613152
##  [911] 0.3390869 0.3379737 0.3417701 0.3430479 0.3512546 0.3597958 0.3326141
##  [918] 0.3652135 0.3612405 0.3307036 0.3482416 0.3306218 0.3346122 0.3558022
##  [925] 0.3602581 0.3237699 0.3268218 0.3359191 0.3438802 0.3436134 0.3447512
##  [932] 0.3573229 0.3287972 0.3358264 0.3477789 0.3458343 0.3471223 0.3405068
##  [939] 0.3555415 0.3675011 0.3491382 0.3284198 0.3561777 0.3444680 0.3455486
##  [946] 0.3524659 0.3278394 0.3416070 0.3462805 0.3450860 0.3498167 0.3519612
##  [953] 0.3383190 0.3517323 0.3441642 0.3478472 0.3481940 0.3361888 0.3681491
##  [960] 0.3407000 0.3317259 0.3488541 0.3372760 0.3587381 0.3403548 0.3577427
##  [967] 0.3426915 0.3544417 0.3391320 0.3675361 0.3286744 0.3380755 0.3357776
##  [974] 0.3319215 0.3487153 0.3577545 0.3478421 0.3438711 0.3491449 0.3438502
##  [981] 0.3459849 0.3503088 0.3485797 0.3486822 0.3520934 0.3312681 0.3708556
##  [988] 0.3261571 0.3483914 0.3442478 0.3451252 0.3327775 0.3393494 0.3438029
##  [995] 0.3331817 0.3222024 0.3370067 0.3566050 0.3542665 0.3396530 0.3233390
## [1002] 0.3548231 0.3343648 0.3718348 0.3244817 0.3327269 0.3513135 0.3429523
## [1009] 0.3463445 0.3455335 0.3588449 0.3307507 0.3161493 0.3354785 0.3503097
## [1016] 0.3268484 0.3474087 0.3640157 0.3425008 0.3407098 0.3338350 0.3494968
## [1023] 0.3377425 0.3402230 0.3381384 0.3447222 0.3468349 0.3433666 0.3395318
## [1030] 0.3450550 0.3490341 0.3578905 0.3535559 0.3323780 0.3551701 0.3430623
## [1037] 0.3411244 0.3646513 0.3516922 0.3395549 0.3367886 0.3429787 0.3435148
## [1044] 0.3502182 0.3454276 0.3487202 0.3479409 0.3426769 0.3400298 0.3480892
## [1051] 0.3387191 0.3578763 0.3295896 0.3573782 0.3398552 0.3510779 0.3391355
## [1058] 0.3348751 0.3418924 0.3515313 0.3360781 0.3535091 0.3403274 0.3449387
## [1065] 0.3493363 0.3414927 0.3475446 0.3374486 0.3408994 0.3429609 0.3428716
## [1072] 0.3561007 0.3404950 0.3480847 0.3330440 0.3419784 0.3451265 0.3388064
## [1079] 0.3572197 0.3492258 0.3316003 0.3278264 0.3514285 0.3384523 0.3550941
## [1086] 0.3417659 0.3471293 0.3478929 0.3435170 0.3410594 0.3443952 0.3478290
## [1093] 0.3251717 0.3562969 0.3619436 0.3312339 0.3626947 0.3347802 0.3546512
## [1100] 0.3461796 0.3670297 0.3234016 0.3676413 0.3264766 0.3244477 0.3535986
## [1107] 0.3325341 0.3483407 0.3421345 0.3481834 0.3381686 0.3422776 0.3662719
## [1114] 0.3353612 0.3506738 0.3518626 0.3468699 0.3502561 0.3370591 0.3376960
## [1121] 0.3363074 0.3414772 0.3428907 0.3379695 0.3379200 0.3490628 0.3472526
## [1128] 0.3494813 0.3492824 0.3281732 0.3435474 0.3556237 0.3409155 0.3480976
## [1135] 0.3469753 0.3380532 0.3484476 0.3488639 0.3349870 0.3488543 0.3533676
## [1142] 0.3330673 0.3550394 0.3405607 0.3438551 0.3443571 0.3401313 0.3411155
## [1149] 0.3606851 0.3593674 0.3394516 0.3582244 0.3374506 0.3588710 0.3412741
## [1156] 0.3600103 0.3354770 0.3424360 0.3467128 0.3483223 0.3439346 0.3414136
## [1163] 0.3345762 0.3397350 0.3345969 0.3533679 0.3536717 0.3589540 0.3349994
## [1170] 0.3554541 0.3317168 0.3464892 0.3493945 0.3591890 0.3478466 0.3546349
## [1177] 0.3597578 0.3252916 0.3637325 0.3332607 0.3505556 0.3435102 0.3394194
## [1184] 0.3498658 0.3419005 0.3421840 0.3470870 0.3355698 0.3566486 0.3448992
## [1191] 0.3544084 0.3515927 0.3477711 0.3445597 0.3355389 0.3505792 0.3556268
## [1198] 0.3347762 0.3665063 0.3280081 0.3495611 0.3542765 0.3445070 0.3497358
## [1205] 0.3596317 0.3220495 0.3198418 0.3429383 0.3490533 0.3515150 0.3543338
## [1212] 0.3286074 0.3288691 0.3635793 0.3664565 0.3614060 0.3538379 0.3652159
## [1219] 0.3209894 0.3506288 0.3477109 0.3520929 0.3368961 0.3525147 0.3390801
## [1226] 0.3446950 0.3418114 0.3523871 0.3484050 0.3494762 0.3518031 0.3498476
## [1233] 0.3420307 0.3459775 0.3575658 0.3464545 0.3558888 0.3381904 0.3509725
## [1240] 0.3355992 0.3609624 0.3534775 0.3366572 0.3564360 0.3310389 0.3322535
## [1247] 0.3516003 0.3421876 0.3640735 0.3610728 0.3465155 0.3377781 0.3382894
## [1254] 0.3471250 0.3475686 0.3440903 0.3397867 0.3434342 0.3455309 0.3372960
## [1261] 0.3378350 0.3560049 0.3564503 0.3558690 0.3556752 0.3454637 0.3508912
## [1268] 0.3421980 0.3423747 0.3585754 0.3339740 0.3521176 0.3333314 0.3314980
## [1275] 0.3211649 0.3452210 0.3368994 0.3519241 0.3313226 0.3418383 0.3539181
## [1282] 0.3542140 0.3468281 0.3507390 0.3410942 0.3504335 0.3219747 0.3311123
## [1289] 0.3403316 0.3487427 0.3289147 0.3439188 0.3406681 0.3419103 0.3523620
## [1296] 0.3291060 0.3309119 0.3325109 0.3557114 0.3525221 0.3429826 0.3430056
## [1303] 0.3467387 0.3438855 0.3544465 0.3584960 0.3506857 0.3460390 0.3554227
## [1310] 0.3689733 0.3268597 0.3681355 0.3280517 0.3539022 0.3523590 0.3367319
## [1317] 0.3432528 0.3461647 0.3566296 0.3436062 0.3409237 0.3233152 0.3425108
## [1324] 0.3399854 0.3555840 0.3261119 0.3327142 0.3517263 0.3438406 0.3516962
## [1331] 0.3518718 0.3552325 0.3686133 0.3683537 0.3633077 0.3208983 0.3657262
## [1338] 0.3393692 0.3391756 0.3550317 0.3443964 0.3480005 0.3468312 0.3530739
## [1345] 0.3406466 0.3469979 0.3442001 0.3405056 0.3471508 0.3448077 0.3499558
## [1352] 0.3385523 0.3466923 0.3521791 0.3358698 0.3458182 0.3546972 0.3440753
## [1359] 0.3436770 0.3500937 0.3445858 0.3548792 0.3341005 0.3429038 0.3585780
## [1366] 0.3386532 0.3413041 0.3456124 0.3438798 0.3444469 0.3465557 0.3525166
## [1373] 0.3436006 0.3420821 0.3455920 0.3525836 0.3449056 0.3467648 0.3583983
## [1380] 0.3471036 0.3421183 0.3561872 0.3367856 0.3369331 0.3465228 0.3435455
## [1387] 0.3410087 0.3359655 0.3586311 0.3347380 0.3484342 0.3468783 0.3338863
## [1394] 0.3572267 0.3434658 0.3455144 0.3452656 0.3414331 0.3593124 0.3352511
## [1401] 0.3470674 0.3378093 0.3530146 0.3531880 0.3516429 0.3525217 0.3391873
## [1408] 0.3395738 0.3434277 0.3436698 0.3486386 0.3416961 0.3477910 0.3481907
## [1415] 0.3527953 0.3605433 0.3522561 0.3414583 0.3510288 0.3555642 0.3380002
## [1422] 0.3438515 0.3496865 0.3398212 0.3380110 0.3525696 0.3475855 0.3445509
## [1429] 0.3403538 0.3396705 0.3367891 0.3421996 0.3436084 0.3383572 0.3465480
## [1436] 0.3407250 0.3513327 0.3319225 0.3485644 0.3465833 0.3481539 0.3508693
## [1443] 0.3254306 0.3559054 0.3510221 0.3614506 0.3328648 0.3352903 0.3565177
## [1450] 0.3516784 0.3412899 0.3517299 0.3340511 0.3543092 0.3357964 0.3518134
## [1457] 0.3518134 0.3424294 0.3520608 0.3355817 0.3567418 0.3347142 0.3523563
## [1464] 0.3307901 0.3339685 0.3568376 0.3353946 0.3516099 0.3230777 0.3244568
## [1471] 0.3694694 0.3574074 0.3507528 0.3373616 0.3405143 0.3400445 0.3363836
## [1478] 0.3485483 0.3419535 0.3512774 0.3401658 0.3409566 0.3554295 0.3306920
## [1485] 0.3622595 0.3419183 0.3433782 0.3415761 0.3297257 0.3363530 0.3786902
## [1492] 0.3774883 0.3358033 0.3327112 0.3519380 0.3348639 0.3445617 0.3459531
## [1499] 0.3492143 0.3391976 0.3336252 0.3345650 0.3495390 0.3567070 0.3573564
## [1506] 0.3526039 0.3443605 0.3397703 0.3322209 0.3627175 0.3285119 0.3651698
## [1513] 0.3289128 0.3450499 0.3531170 0.3358115 0.3358469 0.3727477 0.3271740
## [1520] 0.3631266 0.3659507 0.3325735 0.3427369 0.3520038 0.3532142 0.3468651
## [1527] 0.3420015 0.3548022 0.3467520 0.3324549 0.3429561 0.3508785 0.3434104
## [1534] 0.3496392 0.3456025 0.3385191 0.3355527 0.3345843 0.3259610 0.3513375
## [1541] 0.3450432 0.3341215 0.3580316 0.3467600 0.3550879 0.3335778 0.3357168
## [1548] 0.3635316 0.3540534 0.3350765 0.3729799 0.3372936 0.3400495 0.3388278
## [1555] 0.3408647 0.3307606 0.3373234 0.3469435 0.3373736 0.3639643 0.3610716
## [1562] 0.3334237 0.3550293 0.3344562 0.3580122 0.3473422 0.3397402 0.3494859
## [1569] 0.3544046 0.3361532 0.3425375 0.3510845 0.3338611 0.3384197 0.3396301
## [1576] 0.3436525 0.3328707 0.3589426 0.3560988 0.3434116 0.3642650 0.3372386
## [1583] 0.3489945 0.3209212 0.3639223 0.3366460 0.3508845 0.3505851 0.3278551
## [1590] 0.3548940 0.3546246 0.3418371 0.3332082 0.3478819 0.3389777 0.3421729
## [1597] 0.3300283 0.3401155 0.3423235 0.3527827 0.3284489 0.3523153 0.3520040
## [1604] 0.3439874 0.3560121 0.3334397 0.3463312 0.3488566 0.3409701 0.3447326
## [1611] 0.3463552 0.3466735 0.3464442 0.3493350 0.3458791 0.3450195 0.3475448
## [1618] 0.3346135 0.3284238 0.3493712 0.3560991 0.3411769 0.3419350 0.3351113
## [1625] 0.3375466 0.3559564 0.3352514 0.3374618 0.3475736 0.3241981 0.3245238
## [1632] 0.3428787 0.3391066 0.3270052 0.3243424 0.3609335 0.3539455 0.3291105
## [1639] 0.3573814 0.3370401 0.3537428 0.3425800 0.3513173 0.3369275 0.3433533
## [1646] 0.3360847 0.3349103 0.3631795 0.3336296 0.3579863 0.3398732 0.3392018
## [1653] 0.3521306 0.3443199 0.3517410 0.3374250 0.3535147 0.3467554 0.3485067
## [1660] 0.3313975 0.3503097 0.3470984 0.3227809 0.3435017 0.3534573 0.3400437
## [1667] 0.3608246 0.3385727 0.3390642 0.3415195 0.3389276 0.3582210 0.3267650
## [1674] 0.3708439 0.3230135 0.3270791 0.3599688 0.3331774 0.3277367 0.3554819
## [1681] 0.3483505 0.3500930 0.3432519 0.3444978 0.3389689 0.3468781 0.3530514
## [1688] 0.3388760 0.3511578 0.3350667 0.3405681 0.3393409 0.3534018 0.3617900
## [1695] 0.3206234 0.3594805 0.3607914 0.3352160 0.3436651 0.3447022 0.3338410
## [1702] 0.3688582 0.3336918 0.3592617 0.3457794 0.3409789 0.3609478 0.3422935
## [1709] 0.3522683 0.3419921 0.3445515 0.3533756 0.3476378 0.3538980 0.3384463
## [1716] 0.3399280 0.3430159 0.3464354 0.3649706 0.3261461 0.3564333 0.3422157
## [1723] 0.3300138 0.3496995 0.3413917 0.3355832 0.3254392 0.3256889 0.3359713
## [1730] 0.3168206 0.3291199 0.3509501 0.3500107 0.3425500 0.3352445 0.3290395
## [1737] 0.3216187 0.3807492 0.3699474 0.3191476 0.3506485 0.3454301 0.3466583
## [1744] 0.3474033 0.3305994 0.3330947 0.3250027 0.3619090 0.3638908 0.3222323
## [1751] 0.3702618 0.3423264 0.3441731 0.3564425 0.3380328 0.3317171 0.3436666
## [1758] 0.3337214 0.3425408 0.3461077 0.3302319 0.3351845 0.3275792 0.3626880
## [1765] 0.3469884 0.3431202 0.3471341 0.3424065 0.3517007 0.3350624 0.3574752
## [1772] 0.3428833 0.3545859 0.3515031 0.3347222 0.3689903 0.3330784 0.3360404
## [1779] 0.3553739 0.3368007 0.3350237 0.3539016 0.3476038 0.3435176 0.3444906
## [1786] 0.3427271 0.3291140 0.3420305 0.3532116 0.3431524 0.3438857 0.3392400
## [1793] 0.3524007 0.3608679 0.3418127 0.3371919 0.3587906 0.3321960 0.3473787
## [1800] 0.3350754 0.3519820 0.3484544 0.3462172 0.3503321 0.3312427 0.3555835
## [1807] 0.3389523 0.3517064 0.3517064 0.3381524 0.3558285 0.3384753 0.3546737
## [1814] 0.3459992 0.3398975 0.3373231 0.3549999 0.3389368 0.3305047 0.3408626
## [1821] 0.3559553 0.3295766 0.3385731 0.3472917 0.3457852 0.3470224 0.3557137
## [1828] 0.3534516 0.3523986 0.3339075 0.3494134 0.3449908 0.3301802 0.3545256
## [1835] 0.3379075 0.3389093 0.3447428 0.3519515 0.3327917 0.3524737 0.3343666
## [1842] 0.3461935 0.3451844 0.3433507 0.3460833 0.3351626 0.3474459 0.3415741
## [1849] 0.3434766 0.3510650 0.3439036 0.3491975 0.3707804 0.3580723 0.3287858
## [1856] 0.3397468 0.3449135 0.3527548 0.3438017 0.3455000 0.3341267 0.3377903
## [1863] 0.3352990 0.3576355 0.3285263 0.3463614 0.3458042 0.3469147 0.3435801
## [1870] 0.3392448 0.3375766 0.3567848 0.3597595 0.3484312 0.3306899 0.3639831
## [1877] 0.3528986 0.3485396 0.3537670 0.3396144 0.3418208 0.3452818 0.3347719
## [1884] 0.3441965 0.3461732 0.3414676 0.3614271 0.3333658 0.3426661 0.3425803
## [1891] 0.3403309 0.3272409 0.3585680 0.3331883 0.3585459 0.3627957 0.3557282
## [1898] 0.3411672 0.3447103 0.3453227 0.3589960 0.3402448 0.3522230 0.3495575
## [1905] 0.3474753 0.3440631 0.3510755 0.3569636 0.3469326 0.3537124 0.3388839
## [1912] 0.3455692 0.3501602 0.3433333 0.3516047 0.3487912 0.3345795 0.3686377
## [1919] 0.3325119 0.3398797 0.3416741 0.3402593 0.3582573 0.3337849 0.3381591
## [1926] 0.3411642 0.3487421 0.3514021 0.3337825 0.3297256 0.3308861 0.3545785
## [1933] 0.3371308 0.3490091 0.3405874 0.3552950 0.3520934 0.3427868 0.3216689
## [1940] 0.3465069 0.3450049 0.3484099 0.3502578 0.3517177 0.3399129 0.3380746
## [1947] 0.3451002 0.3483562 0.3389868 0.3379088 0.3349753 0.3369161 0.3520933
## [1954] 0.3336267 0.3519073 0.3426216 0.3516965 0.3280738 0.3385589 0.3347410
## [1961] 0.3292128 0.3283903 0.3318174 0.3614377 0.3289216 0.3356693 0.3506889
## [1968] 0.3509606 0.3251741 0.3512286 0.3417780 0.3454089 0.3417333 0.3637841
## [1975] 0.3278787 0.3253755 0.3648495 0.3322158 0.3243587 0.3588111 0.3544265
## [1982] 0.3432075 0.3583110 0.3369838 0.3508995 0.3500292 0.3721575 0.3565212
## [1989] 0.3573344 0.3296217 0.3618117 0.3428790 0.3377556 0.3383281 0.3481508
## [1996] 0.3413521 0.3602195 0.3555914 0.3586367 0.3277499 0.3532397 0.3337837
## [2003] 0.3362807 0.3532068 0.3200789 0.3206480 0.3616531 0.3426219 0.3442726
## [2010] 0.3485585 0.3494693 0.3345640 0.3482053 0.3627589 0.3321748 0.3632832
## [2017] 0.3331775 0.3514452 0.3399921 0.3628974 0.3351199 0.3388439 0.3444450
## [2024] 0.3538094 0.3384861 0.3525985 0.3404880 0.3349891 0.3462306 0.3467355
## [2031] 0.3420529 0.3424085 0.3553089 0.3375403 0.3340875 0.3588012 0.3351447
## [2038] 0.3547706 0.3516148 0.3316761 0.3624413 0.3338916 0.3620482 0.3674139
## [2045] 0.3337909 0.3453482 0.3440571 0.3542372 0.3390469 0.3516659 0.3513108
## [2052] 0.3502364 0.3366648 0.3345396 0.3394549 0.3520443 0.3466906 0.3583057
## [2059] 0.3424951 0.3435913 0.3422650 0.3471507 0.3510831 0.3501016 0.3384174
## [2066] 0.3479335 0.3386813 0.3511499 0.3367397 0.3550197 0.3519286 0.3306996
## [2073] 0.3286268 0.3502083 0.3394912 0.3510235 0.3669732 0.3638972 0.3258658
## [2080] 0.3620494 0.3497700 0.3487186 0.3499853 0.3637877 0.3272987 0.3620071
## [2087] 0.3489148 0.3496335 0.3475448 0.3309294 0.3464208 0.3475695 0.3393854
## [2094] 0.3427202 0.3437107 0.3313926 0.3412246 0.3218870 0.3263941 0.3517570
## [2101] 0.3437229 0.3598185 0.3511460 0.3450442 0.3433133 0.3488679 0.3485042
## [2108] 0.3408745 0.3444166 0.3320920 0.3340531 0.3239944 0.3414172 0.3605301
## [2115] 0.3615954 0.3428591 0.3370182 0.3380499 0.3232804 0.3517522 0.3325696
## [2122] 0.3581249 0.3541741 0.3553420 0.3503188 0.3432788 0.3493801 0.3358731
## [2129] 0.3496535 0.3437608 0.3464305 0.3476325 0.3446803 0.3701976 0.3480032
## [2136] 0.3462370 0.3447826 0.3280715 0.3634310 0.3654435 0.3647801 0.3595245
## [2143] 0.3315779 0.3465211 0.3379357 0.3550985 0.3289978 0.3335863 0.3316622
## [2150] 0.3669127 0.3358727 0.3534117 0.3404353 0.3654704 0.3620557 0.3271177
## [2157] 0.3484096 0.3436012 0.3303104 0.3649302 0.3352928 0.3490233 0.3657223
## [2164] 0.3642785 0.3257994 0.3580912 0.3470822 0.3408988 0.3321856 0.3589023
## [2171] 0.3384330 0.3515411 0.3424050 0.3399032 0.3449707 0.3490151 0.3381867
## [2178] 0.3509300 0.3481669 0.3472474 0.3392274 0.3452566 0.3422156 0.3331814
## [2185] 0.3280864 0.3581751 0.3562125 0.3487062 0.3492516 0.3388495 0.3552467
## [2192] 0.3334576 0.3630136 0.3256957 0.3199891 0.3595911 0.3489471 0.3515937
## [2199] 0.3434330 0.3429198 0.3586555 0.3404845 0.3638548 0.3575315 0.3429974
## [2206] 0.3414879 0.3276151 0.3713340 0.3258722 0.3378796 0.3503221 0.3279252
## [2213] 0.3539322 0.3359297 0.3484778 0.3451954 0.3363537 0.3570056 0.3326026
## [2220] 0.3580377 0.3487901 0.3457995 0.3352867 0.3434052 0.3438984 0.3550533
## [2227] 0.3511452 0.3405518 0.3472649 0.3248111 0.3725481 0.3725481 0.3571280
## [2234] 0.3460524 0.3424851 0.3342084 0.3463213 0.3537300 0.3354137 0.3550168
## [2241] 0.3415113 0.3428086 0.3469208 0.3358122 0.3333295 0.3570005 0.3626281
## [2248] 0.3445463 0.3609936 0.3421107 0.3461625 0.3429170 0.3588258 0.3600035
## [2255] 0.3430862 0.3390747 0.3323620 0.3233949 0.3206717 0.3369495 0.3364767
## [2262] 0.3581490 0.3514078 0.3494167 0.3423949 0.3455977 0.3431214 0.3487142
## [2269] 0.3385787 0.3590786 0.3417125 0.3191365 0.3435502 0.3643291 0.3361917
## [2276] 0.3581829 0.3543990 0.3392918 0.3401531 0.3469481 0.3497271 0.3415478
## [2283] 0.3393546 0.3494876 0.3520636 0.3515614 0.3420804 0.3567433 0.3743271
## [2290] 0.3175628 0.3466262 0.3354089 0.3582502 0.3530055 0.3298851 0.3566359
## [2297] 0.3345852 0.3554734 0.3632173 0.3272043 0.3369711 0.3470840 0.3509076
## [2304] 0.3555031 0.3516393 0.3380933 0.3398645 0.3464643 0.3451646 0.3642983
## [2311] 0.3339316 0.3417556 0.3543600 0.3594863 0.3482514 0.3417669 0.3442821
## [2318] 0.3420666 0.3260064 0.3542991 0.3587165 0.3308879 0.3551241 0.3622027
## [2325] 0.3293852 0.3313311 0.3529236 0.3580376 0.3544205 0.3309228 0.3308145
## [2332] 0.3312645 0.3435395 0.3473060 0.3453398 0.3457638 0.3413018 0.3420592
## [2339] 0.3705879 0.3218137 0.3378720 0.3453152 0.3457209 0.3413450 0.3459481
## [2346] 0.3251565 0.3593599 0.3500218 0.3306406 0.3556403 0.3460031 0.3374928
## [2353] 0.3576055 0.3501602 0.3422540 0.3428542 0.3566382 0.3474026 0.3453288
## [2360] 0.3307279 0.3252189 0.3288225 0.3638766 0.3279902 0.3574186 0.3267193
## [2367] 0.3495479 0.3423578 0.3531071 0.3553015 0.3526833 0.3337305 0.3424246
## [2374] 0.3353233 0.3640811 0.3547009 0.3432190 0.3510884 0.3447173 0.3485305
## [2381] 0.3539054 0.3470586 0.3449451 0.3528896 0.3320745 0.3613307 0.3638610
## [2388] 0.3440578 0.3507562 0.3438154 0.3571756 0.3501569 0.3364116 0.3650746
## [2395] 0.3588406 0.3550368 0.3361693 0.3372237 0.3483102 0.3443242 0.3378640
## [2402] 0.3523720 0.3532853 0.3328377 0.3596967 0.3618431 0.3326433 0.3345393
## [2409] 0.3615818 0.3424982 0.3464498 0.3430879 0.3431053 0.3411306 0.3271112
## [2416] 0.3223903 0.3515813 0.3343894 0.3383414 0.3497266 0.3369148 0.3499047
## [2423] 0.3458012 0.3406969 0.3415590 0.3522489 0.3451459 0.3489233 0.3526558
## [2430] 0.3303224 0.3628120 0.3425071 0.3502654 0.3511457 0.3477375 0.3431220
## [2437] 0.3436794 0.3333854 0.3501386 0.3492604 0.3504327 0.3361447 0.3326037
## [2444] 0.3672211 0.3415971 0.3510463 0.3518230 0.3559147 0.3410516 0.3267140
## [2451] 0.3521504 0.3494520 0.3439146 0.3543455 0.3391047 0.3457106 0.3447162
## [2458] 0.3576161 0.3497971 0.3409115 0.3287051 0.3573012 0.3450626 0.3420567
## [2465] 0.3442376 0.3483118 0.3453887 0.3362472 0.3468915 0.3435593 0.3463598
## [2472] 0.3353671 0.3480360 0.3459787 0.3374061 0.3547480 0.3464630 0.3466605
## [2479] 0.3426552 0.3483135 0.3519249 0.3405614 0.3459582 0.3481913 0.3461194
## [2486] 0.3366231 0.3332897 0.3520245 0.3314070 0.3332449 0.3346679 0.3312163
## [2493] 0.3372977 0.3500160 0.3576077 0.3512153 0.3430377 0.3449520 0.3511515
## [2500] 0.3316219 0.3389088 0.3538812 0.3612720 0.3350089 0.3539284 0.3427257
## [2507] 0.3412043 0.3710017 0.3465730 0.3444947 0.3384188 0.3408786 0.3523337
## [2514] 0.3430542 0.3543943 0.3477375 0.3493947 0.3527662 0.3651647 0.3227847
## [2521] 0.3450188 0.3319655 0.3432073 0.3466185 0.3490631 0.3502544 0.3523806
## [2528] 0.3290435 0.3361420 0.3309073 0.3577248 0.3503453 0.3139990 0.3726735
## [2535] 0.3216830 0.3188249 0.3199447 0.3720842 0.3285960 0.3552447 0.3404969
## [2542] 0.3374586 0.3490150 0.3406630 0.3534465 0.3464933 0.3447164 0.3466199
## [2549] 0.3591422 0.3440671 0.3366747 0.3415968 0.3454460 0.3578182 0.3399371
## [2556] 0.3431717 0.3355583 0.3455998 0.3535555 0.3595080 0.3355783 0.3487820
## [2563] 0.3411447 0.3464727 0.3345974 0.3514457 0.3510362 0.3310596 0.3537602
## [2570] 0.3420082 0.3423899 0.3469866 0.3417524 0.3427107 0.3495071 0.3499293
## [2577] 0.3391661 0.3483050 0.3606892 0.3291654 0.3495892 0.3315023 0.3609620
## [2584] 0.3412795 0.3511300 0.3359791 0.3370824 0.3438350 0.3425819 0.3476113
## [2591] 0.3455056 0.3496765 0.3508911 0.3554734 0.3214013 0.3351732 0.3443018
## [2598] 0.3444164 0.3483260 0.3557914 0.3275033 0.3550809 0.3461956 0.3555803
## [2605] 0.3666874 0.3516240 0.3600465 0.3607716 0.3623563 0.3156781 0.3669702
## [2612] 0.3372866 0.3480356 0.3661310 0.3588751 0.3402287 0.3469125 0.3478611
## [2619] 0.3376331 0.3383696 0.3385140 0.3482771 0.3443634 0.3326896 0.3484089
## [2626] 0.3460183 0.3549029 0.3364330 0.3402666 0.3590033 0.3303390 0.3633768
## [2633] 0.3379449 0.3376796 0.3350869 0.3705690 0.3308855 0.3263287 0.3417885
## [2640] 0.3369570 0.3504316 0.3401896 0.3466501 0.3498103 0.3333920 0.3659622
## [2647] 0.3585973 0.3394725 0.3423704 0.3430597 0.3525063 0.3401235 0.3397710
## [2654] 0.3363760 0.3454059 0.3453157 0.3480463 0.3553366 0.3637836 0.3323599
## [2661] 0.3291411 0.3634387 0.3549211 0.3419964 0.3565056 0.3381196 0.3317935
## [2668] 0.3638600 0.3270660 0.3467614 0.3520678 0.3432854 0.3471312 0.3341588
## [2675] 0.3341332 0.3234252 0.3662624 0.3659965 0.3343818 0.3583182 0.3551361
## [2682] 0.3219908 0.3378357 0.3509217 0.3532888 0.3494431 0.3426599 0.3655803
## [2689] 0.3219220 0.3620564 0.3426004 0.3554698 0.3267015 0.3416733 0.3509560
## [2696] 0.3402241 0.3563472 0.3666143 0.3585902 0.3557046 0.3494196 0.3472265
## [2703] 0.3352911 0.3538928 0.3448524 0.3477039 0.3442786 0.3468158 0.3558534
## [2710] 0.3242137 0.3567950 0.3312671 0.3403006 0.3621669 0.3259990 0.3547169
## [2717] 0.3522871 0.3401533 0.3601466 0.3602078 0.3521329 0.3441540 0.3406337
## [2724] 0.3448682 0.3692414 0.3785538 0.3739899 0.3674931 0.3257677 0.3553833
## [2731] 0.3420730 0.3475065 0.3458419 0.3469288 0.3534428 0.3393610 0.3328022
## [2738] 0.3336913 0.3558554 0.3466030 0.3549813 0.3385287 0.3658416 0.3629759
## [2745] 0.3535436 0.3557208 0.3465645 0.3556951 0.3511730 0.3349094 0.3381023
## [2752] 0.3508259 0.3393949 0.3475752 0.3484829 0.3397923 0.3467244 0.3492213
## [2759] 0.3401969 0.3575927 0.3507571 0.3465492 0.3423074 0.3433240 0.3399333
## [2766] 0.3387824 0.3526535 0.3465255 0.3377284 0.3540641 0.3557704 0.3427740
## [2773] 0.3347448 0.3479603 0.3431012 0.3581152 0.3664820 0.3376505 0.3519324
## [2780] 0.3554221 0.3322793 0.3509345 0.3327525 0.3563240 0.3435724 0.3467609
## [2787] 0.3554098 0.3580372 0.3421726 0.3511157 0.3447174 0.3344210 0.3386334
## [2794] 0.3526659 0.3375215 0.3350572 0.3231284 0.3712040 0.3703026 0.3462542
## [2801] 0.3492063 0.3378668 0.3571656 0.3457856 0.3535901 0.3367700 0.3486067
## [2808] 0.3614473 0.3373529 0.3567727 0.3485597 0.3268124 0.3530145 0.3418468
## [2815] 0.3411942 0.3462113 0.3534006 0.3344893 0.3455933 0.3439889 0.3449592
## [2822] 0.3577181 0.3332192 0.3323570 0.3506932 0.3451156 0.3445045 0.3359362
## [2829] 0.3573638 0.3372885 0.3509350 0.3346353 0.3453163 0.3374866 0.3393581
## [2836] 0.3326024 0.3543507 0.3425904 0.3449889 0.3491173 0.3386305 0.3510916
## [2843] 0.3530757 0.3451351 0.3478396 0.3536297 0.3564746 0.3498834 0.3592333
## [2850] 0.3681761 0.3607505 0.3236909 0.3185666 0.3540052 0.3389298 0.3445758
## [2857] 0.3499097 0.3423776 0.3561060 0.3401980 0.3588493 0.3569166 0.3613544
## [2864] 0.3308663 0.3550070 0.3474487 0.3481084 0.3455874 0.3499459 0.3455107
## [2871] 0.3410958 0.3400146 0.3295968 0.3486486 0.3411126 0.3540195 0.3473641
## [2878] 0.3486903 0.3274717 0.3550318 0.3605624 0.3423003 0.3350815 0.3433487
## [2885] 0.3493320 0.3589039 0.3713269 0.3522673 0.3307378 0.3331129 0.3442848
## [2892] 0.3515122 0.3341531 0.3459471 0.3497256 0.3498747 0.3529062 0.3419064
## [2899] 0.3436473 0.3476966 0.3428311 0.3577759 0.3381811 0.3440772 0.3420963
## [2906] 0.3448232 0.3197713 0.3725409 0.3444942 0.3377968 0.3464913 0.3471874
## [2913] 0.3510709 0.3499736 0.3686999 0.3362451 0.3571848 0.3629038 0.3566853
## [2920] 0.3555623 0.3292600 0.3364428 0.3348933 0.3207929 0.3282530 0.3612201
## [2927] 0.3395744 0.3427363 0.3500969 0.3601828 0.3480417 0.3493247 0.3320846
## [2934] 0.3557878 0.3376670 0.3556590 0.3396713 0.3472963 0.3341583 0.3621657
## [2941] 0.3560309 0.3653788 0.3365717 0.3514035 0.3355143 0.3377441 0.3355058
## [2948] 0.3521506 0.3412061 0.3431484 0.3195676 0.3654261 0.3556109 0.3595392
## [2955] 0.3253529 0.3606819 0.3710054 0.3219201 0.3594122 0.3276119 0.3685310
## [2962] 0.3630138 0.3243754 0.3613132 0.3414995 0.3568183 0.3386584 0.3558561
## [2969] 0.3397163 0.3482469 0.3459767 0.3563004 0.3315314 0.3320690 0.3703554
## [2976] 0.3514350 0.3559400 0.3406747 0.3436812 0.3279585 0.3607554 0.3312655
## [2983] 0.3534318 0.3379308 0.3395759 0.3506016 0.3469764 0.3406690 0.3346643
## [2990] 0.3487592 0.3439574 0.3471515 0.3481173 0.3411995 0.3551460 0.3595283
## [2997] 0.3323930 0.3378361 0.3267094 0.3409995 0.3406713 0.3395480 0.3379175
## [3004] 0.3560634 0.3323644 0.3356927 0.3475744 0.3366849 0.3471293 0.3435875
## [3011] 0.3397436 0.3425354 0.3465845 0.3443610 0.3321268 0.3494080 0.3593134
## [3018] 0.3424595 0.3540073 0.3516173 0.3511390 0.3427585 0.3507686 0.3334671
## [3025] 0.3475018 0.3505103 0.3488744 0.3402399 0.3336006 0.3316283 0.3458499
## [3032] 0.3560996 0.3446310 0.3484588 0.3654159 0.3322904 0.3521142 0.3369477
## [3039] 0.3549789 0.3337654 0.3304464 0.3316128 0.3527502 0.3655532 0.3556266
## [3046] 0.3433363 0.3378735 0.3488034 0.3321173 0.3478791 0.3337302 0.3418514
## [3053] 0.3426834 0.3392127 0.3479616 0.3385055 0.3548745 0.3324975 0.3626205
## [3060] 0.3545044 0.3347532 0.3390214 0.3396539 0.3474450 0.3559490 0.3330963
## [3067] 0.3488044 0.3337808 0.3375567 0.3561831 0.3236903 0.3339448 0.3578856
## [3074] 0.3303687 0.3322259 0.3503445 0.3518915 0.3593151 0.3370965 0.3565206
## [3081] 0.3468795 0.3438883 0.3316630 0.3292552 0.3583639 0.3291927 0.3345314
## [3088] 0.3622982 0.3553962 0.3386864 0.3485318 0.3454889 0.3509938 0.3463479
## [3095] 0.3248830 0.3206701 0.3686005 0.3301685 0.3588971 0.3274702 0.3556459
## [3102] 0.3555449 0.3425082 0.3505645 0.3512533 0.3456804 0.3577235 0.3326324
## [3109] 0.3426850 0.3619591 0.3320628 0.3461759 0.3320140 0.3591764 0.3351826
## [3116] 0.3484695 0.3324816 0.3487033 0.3405193 0.3442709 0.3305139 0.3342681
## [3123] 0.3371717 0.3427975 0.3417282 0.3451557 0.3404331 0.3532325 0.3418260
## [3130] 0.3559096 0.3293969 0.3554427 0.3574638 0.3460260 0.3439461 0.3547250
## [3137] 0.3281241 0.3517925 0.3412281 0.3443872 0.3369993 0.3618651 0.3379564
## [3144] 0.3252424 0.3665738 0.3636845 0.3522424 0.3359765 0.3540378 0.3795045
## [3151] 0.3193265 0.3651002 0.3230777 0.3756730 0.3281429 0.3535493 0.3373279
## [3158] 0.3319118 0.3463292 0.3548165 0.3548446 0.3508212 0.3516773 0.3562753
## [3165] 0.3327833 0.3498903 0.3361051 0.3512310 0.3465798 0.3395227 0.3580480
## [3172] 0.3333760 0.3555541 0.3501586 0.3541005 0.3289086 0.3490021 0.3433007
## [3179] 0.3731571 0.3386984 0.3513343 0.3542269 0.3368346 0.3462416 0.3374300
## [3186] 0.3512254 0.3524009 0.3386935 0.3533540 0.3495589 0.3330058 0.3145126
## [3193] 0.3282330 0.3460855 0.3385287 0.3372968 0.3461822 0.3532578 0.3465781
## [3200] 0.3517128 0.3441287 0.3444570 0.3329361 0.3398227 0.3533178 0.3447715
## [3207] 0.3492495 0.3480523 0.3445181 0.3547903 0.3494578 0.3395769 0.3442781
## [3214] 0.3406685 0.3343426 0.3436609 0.3377872 0.3523151 0.3448670 0.3472380
## [3221] 0.3203406 0.3268443 0.3345789 0.3254853 0.3357163 0.3655740 0.3747807
## [3228] 0.3627669 0.3474752 0.3456237 0.3463291 0.3502207 0.3331727 0.3591415
## [3235] 0.3414145 0.3296825 0.3191545 0.3385510 0.3345295 0.3383803 0.3398946
## [3242] 0.3586180 0.3406842 0.3216194 0.3391668 0.3411286 0.3503730 0.3437331
## [3249] 0.3442581 0.3419944 0.3477880 0.3408740 0.3455519 0.3429342 0.3540177
## [3256] 0.3330717 0.3409382 0.3583912 0.3517566 0.3502060 0.3419716 0.3459750
## [3263] 0.3466866 0.3443154 0.3499124 0.3420299 0.3516591 0.3469862 0.3471347
## [3270] 0.3417621 0.3465453 0.3484198 0.3296615 0.3335227 0.3482698 0.3280934
## [3277] 0.3386775 0.3288725 0.3345639 0.3469713 0.3461395 0.3344966 0.3581492
## [3284] 0.3394534 0.3523472 0.3441895 0.3511264 0.3566622 0.3296635 0.3265867
## [3291] 0.3085171 0.3195190 0.3268487 0.3278715 0.3430779 0.3438383 0.3632566
## [3298] 0.3247650 0.3595250 0.3576527 0.3519559 0.3504628 0.3372299 0.3410511
## [3305] 0.3685031 0.3661440 0.3352390 0.3334416 0.3575716 0.3363170 0.3378899
## [3312] 0.3359903 0.3594611 0.3530671 0.3446310 0.3404750 0.3260934 0.3648195
## [3319] 0.3248682 0.3259377 0.3479738 0.3339773 0.3427631 0.3394147 0.3421052
## [3326] 0.3446406 0.3493418 0.3405537 0.3420013 0.3592250 0.3560084 0.3213188
## [3333] 0.3314153 0.3478784 0.3369267 0.3487375 0.3410359 0.3431796 0.3286226
## [3340] 0.3439338 0.3472262 0.3522118 0.3505403 0.3418123 0.3572546 0.3352372
## [3347] 0.3526720 0.3364798 0.3516885 0.3443088 0.3553845 0.3488308 0.3505462
## [3354] 0.3381744 0.3531500 0.3438861 0.3422818 0.3517097 0.3342613 0.3379265
## [3361] 0.3605857 0.3413807 0.3299072 0.3624305 0.3530257 0.3387000 0.3523545
## [3368] 0.3427050 0.3491739 0.3486785 0.3575056 0.3527401 0.3437458 0.3498896
## [3375] 0.3456442 0.3297014 0.3211475 0.3557636 0.3480398 0.3470545 0.3439604
## [3382] 0.3402403 0.3561497 0.3534161 0.3401583 0.3305248 0.3695481 0.3597906
## [3389] 0.3491864 0.3302485 0.3551647 0.3355033 0.3383042 0.3369575 0.3299571
## [3396] 0.3255960 0.3533614 0.3468834 0.3441592 0.3425512 0.3382551 0.3549653
## [3403] 0.3567589 0.3432815 0.3523891 0.3393346 0.3432031 0.3494990 0.3345337
## [3410] 0.3505772 0.3426235 0.3426130 0.3461110 0.3489962 0.3410947 0.3506885
## [3417] 0.3530183 0.3438058 0.3507239 0.3476652 0.3385470 0.3405989 0.3468703
## [3424] 0.3524316 0.3466730 0.3379447 0.3518626 0.3397207 0.3700165 0.3390686
## [3431] 0.3615672 0.3637489 0.3627935 0.3366528 0.3551540 0.3306607 0.3341153
## [3438] 0.3528129 0.3493983 0.3391070 0.3542970 0.3524347 0.3466389 0.3366369
## [3445] 0.3486345 0.3511834 0.3656610 0.3542543 0.3599844 0.3605685 0.3497986
## [3452] 0.3536980 0.3342037 0.3342340 0.3277228 0.3425717 0.3395349 0.3560050
## [3459] 0.3305053 0.3375388 0.3524353 0.3374690 0.3408738 0.3318407 0.3266624
## [3466] 0.3404445 0.3502476 0.3502738 0.3570463 0.3542203 0.3379351 0.3321787
## [3473] 0.3487888 0.3400563 0.3480397 0.3437338 0.3474775 0.3250203 0.3647931
## [3480] 0.3227226 0.3667202 0.3319653 0.3724315 0.3353247 0.3601891 0.3305785
## [3487] 0.3545220 0.3357426 0.3324440 0.3503704 0.3503704 0.3418327 0.3443571
## [3494] 0.3414045 0.3538714 0.3435548 0.3413095 0.3732587 0.3754273 0.3169887
## [3501] 0.3348435 0.3371607 0.3470071 0.3442139 0.3395322 0.3441211 0.3354559
## [3508] 0.3369854 0.3549771 0.3384208 0.3516357 0.3490061 0.3495169 0.3251238
## [3515] 0.3399995 0.3447324 0.3524519 0.3651945 0.3261292 0.3502007 0.3432625
## [3522] 0.3404558 0.3534271 0.3320473 0.3533814 0.3481514 0.3682549 0.3387826
## [3529] 0.3472903 0.3325729 0.3279204 0.3365073 0.3380677 0.3690202 0.3726742
## [3536] 0.3364619 0.3380665 0.3397655 0.3528717 0.3518368 0.3444191 0.3455050
## [3543] 0.3451090 0.3374513 0.3626094 0.3593232 0.3635587 0.3489358 0.3545958
## [3550] 0.3587177 0.3259921 0.3521442 0.3305469 0.3586918 0.3626593 0.3208389
## [3557] 0.3577724 0.3688201 0.3303253 0.3649131 0.3651941 0.3886303 0.3351333
## [3564] 0.3533522 0.3346569 0.3437581 0.3482592 0.3137219 0.3240989 0.3451068
## [3571] 0.3365673 0.3580822 0.3599186 0.3268289 0.3268488 0.3254741 0.3523001
## [3578] 0.3559685 0.3290220 0.3515029 0.3253107 0.3646905 0.3342857 0.3405070
## [3585] 0.3406248 0.3427917 0.3544244 0.3454454 0.3519784 0.3418800 0.3318966
## [3592] 0.3406765 0.3371929 0.3307762 0.3459596 0.3567992 0.3352672 0.3436232
## [3599] 0.3269787 0.3560096 0.3379316 0.3582834 0.3630983 0.3453189 0.3263806
## [3606] 0.3658440 0.3594594 0.3447933 0.3309959 0.3515428 0.3358226 0.3320227
## [3613] 0.3491726 0.3443181 0.3443181 0.3389076 0.3418048 0.3481682 0.3467096
## [3620] 0.3396034 0.3634237 0.3657624 0.3313627 0.3429554 0.3464684 0.3504682
## [3627] 0.3497332 0.3327093 0.3370245 0.3276912 0.3299998 0.3595213 0.3311351
## [3634] 0.3479278 0.3557209 0.3307347 0.3576724 0.3384860 0.3476971 0.3477070
## [3641] 0.3413428 0.3477238 0.3345541 0.3566303 0.3340889 0.3449361 0.3426956
## [3648] 0.3243750 0.3527248 0.3517695 0.3353576 0.3460270 0.3395427 0.3485840
## [3655] 0.3438132 0.3419471 0.3611354 0.3347536 0.3425786 0.3398543 0.3385008
## [3662] 0.3405150 0.3538008 0.3395367 0.3386944 0.3360647 0.3460381 0.3468931
## [3669] 0.3510154 0.3380855 0.3650770 0.3213531 0.3413357 0.3520465 0.3491939
## [3676] 0.3505575 0.3439891 0.3421066 0.3394386 0.3443115 0.3519327 0.3473977
## [3683] 0.3500197 0.3325141 0.3460503 0.3340198 0.3557348 0.3484066 0.3486238
## [3690] 0.3441920 0.3429714 0.3286785 0.3420402 0.3432392 0.3421745 0.3447584
## [3697] 0.3456207 0.3309634 0.3582198 0.3392736 0.3444361 0.3554830 0.3371499
## [3704] 0.3450036 0.3508773 0.3459530 0.3402566 0.3335473 0.3572283 0.3467615
## [3711] 0.3602217 0.3550558 0.3505288 0.3445360 0.3424728 0.3455287 0.3475993
## [3718] 0.3360541 0.3458549 0.3482904 0.3449997 0.3473525 0.3451004 0.3513606
## [3725] 0.3667818 0.3639551 0.3304292 0.3577650 0.3434177 0.3396314 0.3569391
## [3732] 0.3347061 0.3386269 0.3433470 0.3444694 0.3501497 0.3446722 0.3553975
## [3739] 0.3426174 0.3456666 0.3376645 0.3580518 0.3570483 0.3606229 0.3551226
## [3746] 0.3485128 0.3586534 0.3621536 0.3314291 0.3676770 0.3152900 0.3390696
## [3753] 0.3406707 0.3550202 0.3475878 0.3533758 0.3540530 0.3450448 0.3300122
## [3760] 0.3514954 0.3411345 0.3617389 0.3649321 0.3311764 0.3552889 0.3471068
## [3767] 0.3420352 0.3507283 0.3322454 0.3445384 0.3635543 0.3267470 0.3632237
## [3774] 0.3638668 0.3367974 0.3532920 0.3391208 0.3650984 0.3209625 0.3418793
## [3781] 0.3391843 0.3425699 0.3268595 0.3649234 0.3372381 0.3518380 0.3528785
## [3788] 0.3397949 0.3470665 0.3409979 0.3199365 0.3470682 0.3505255 0.3550147
## [3795] 0.3480593 0.3335695 0.3483833 0.3365716 0.3483430 0.3588070 0.3433266
## [3802] 0.3495118 0.3532797 0.3555985 0.3407739 0.3564441 0.3540831 0.3387078
## [3809] 0.3521940 0.3496590 0.3397612 0.3529989 0.3353944 0.3407668 0.3583086
## [3816] 0.3473010 0.3522848 0.3348452 0.3423879 0.3405422 0.3387994 0.3561683
## [3823] 0.3476177 0.3564849 0.3519632 0.3480674 0.3537446 0.3417451 0.3439062
## [3830] 0.3443412 0.3467373 0.3462594 0.3407986 0.3575641 0.3455989 0.3396646
## [3837] 0.3384608 0.3667006 0.3658855 0.3767223 0.3297736 0.3313063 0.3301227
## [3844] 0.3590984 0.3353367 0.3452998 0.3390760 0.3400859 0.3451183 0.3425801
## [3851] 0.3360337 0.3638190 0.3300855 0.3533192 0.3355085 0.3328528 0.3383633
## [3858] 0.3513304 0.3562957 0.3301145 0.3629470 0.3354644 0.3406098 0.3495361
## [3865] 0.3523877 0.3544571 0.3249491 0.3173648 0.3563700 0.3314886 0.3638477
## [3872] 0.3543140 0.3442008 0.3574708 0.3356429 0.3481050 0.3434237 0.3389582
## [3879] 0.3478035 0.3364088 0.3529549 0.3423897 0.3461899 0.3575507 0.3397326
## [3886] 0.3563333 0.3499475 0.3370951 0.3649880 0.3478905 0.3317474 0.3577872
## [3893] 0.3377090 0.3456969 0.3449936 0.3394243 0.3494301 0.3375560 0.3436209
## [3900] 0.3353977 0.3586252 0.3312935 0.3364020 0.3527828 0.3401757 0.3328492
## [3907] 0.3496877 0.3335009 0.3490880 0.3438103 0.3401867 0.3434693 0.3305326
## [3914] 0.3479252 0.3397481 0.3499387 0.3401819 0.3406124 0.3310999 0.3502612
## [3921] 0.3596650 0.3440644 0.3446859 0.3373675 0.3546105 0.3456746 0.3414705
## [3928] 0.3606027 0.3443820 0.3466852 0.3358632 0.3230513 0.3395138 0.3477067
## [3935] 0.3468478 0.3466908 0.3571689 0.3474117 0.3397234 0.3663592 0.3420785
## [3942] 0.3464036 0.3525285 0.3549488 0.3491503 0.3350112 0.3395647 0.3548241
## [3949] 0.3414965 0.3400377 0.3540270 0.3380304 0.3505654 0.3449850 0.3454054
## [3956] 0.3487394 0.3408347 0.3525970 0.3631148 0.3405077 0.3377790 0.3584293
## [3963] 0.3587872 0.3574772 0.3375295 0.3531378 0.3429979 0.3419810 0.3431147
## [3970] 0.3601736 0.3562567 0.3480485 0.3460609 0.3478982 0.3484968 0.3482185
## [3977] 0.3454552 0.3456836 0.3423258 0.3517039 0.3511123 0.3431466 0.3448517
## [3984] 0.3518673 0.3407838 0.3508955 0.3444783 0.3321858 0.3427402 0.3534972
## [3991] 0.3627731 0.3418650 0.3416328 0.3438581 0.3414223 0.3410847 0.3406371
## [3998] 0.3392088 0.3463930 0.3458904
```

---


``` r
summary(bayes_R2(fit))
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.3085  0.3379  0.3450  0.3452  0.3523  0.3886
```

---

### **Warning Signs**
- `\(R^2\)` &gt; 0.9 in social science: suspect too many variables or close to saturation
- `\(R^2\)` changes dramatically with small changes: suspect multicollinearity or missing data issues

---
# Practical Workflow

## **The Diagnostic Pipeline**

### **Step 1: Before Modeling**
1. **Theory-driven variable selection**
2. **Check data quality**
3. **Visualize relationships**
4. **Consider transformations**

---

### **Step 2: During Modeling**
1. **Try to start with the most complicated specification you might need and simplify down with tests rather than the other way around**
2. **Examine residuals**
3. **Test for nonlinearities**

---

### **Step 3: After Modeling**
1. **Posterior predictive checks**
2. **Cross-validation**
3. **Sensitivity analysis**
4. **Compare to simpler models**

---

### **Step 4: Reporting**
1. **Transparency about assumptions**
2. **Diagnostic results**
3. **Model limitations**
4. **Robustness checks**

```

---

![Diagnostics1](Images/Silva1.png)

---

![Diagnostics2](Images/Silva2.png)


---

![Diagnostics3](Images/Silva3.png)


---

![Diagnostics4](Images/Silva4.png){width=50%}
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
