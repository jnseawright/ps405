<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>12: Multiple Comparisons.</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jaye Seawright" />
    <meta name="date" content="2026-02-16" />
    <script src="MultipleComparisons_files/header-attrs-2.30/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 12: Multiple Comparisons.
]
.subtitle[
## Linear Models
]
.author[
### <large>Jaye Seawright</large>
]
.institute[
### <small>Northwestern Political Science</small>
]
.date[
### Feb. 16, 2026
]

---

class: center, middle

&lt;style type="text/css"&gt;
pre {
  max-height: 400px;
  overflow-y: auto;
}

pre[class] {
  max-height: 200px;
}
&lt;/style&gt;




Last time, we saw that researchers test regression results by rejecting `\(H_{0}\)` when the probability associated with it is very low. By convention, there are thresholds at 0.05, 0.01, and 0.001 about which people get emotional.

---

Consider the probability of getting a heads on a coin toss.

Now consider [this](https://www.youtube.com/watch?v=gOwLEVQGbrM).

---


``` r
set.seed(42)  # For reproducibility
single_experiment &lt;- rbinom(17, size = 1, prob = 0.5)
cat("Number of heads in one experiment:", sum(single_experiment), "\n")
```

```
## Number of heads in one experiment: 12
```

``` r
cat("Probability of 17 heads: ", 0.5^17, " (1 in", round(1/0.5^17), ")\n")
```

```
## Probability of 17 heads:  7.629395e-06  (1 in 131072 )
```

---

All right, but what if I flip the coin as many times as I want and count streaks after the fact?


``` r
set.seed(123)
found_perfect_streak &lt;- FALSE
experiments_run &lt;- 0

while(!found_perfect_streak) {
  experiments_run &lt;- experiments_run + 1
  current_experiment &lt;- rbinom(17, size = 1, prob = 0.5)
  if(sum(current_experiment) == 17) {
    found_perfect_streak &lt;- TRUE
    cat("Found 17 heads after", experiments_run, "experiments\n")
    cat("That's", experiments_run * 17, "total coin flips\n")
  }
}
```

```
## Found 17 heads after 57621 experiments
## That's 979557 total coin flips
```

---


``` r
set.seed(42)
n_experiments &lt;- 1000
results &lt;- replicate(n_experiments, sum(rbinom(17, 1, 0.5)))

max_heads &lt;- max(results)
cat("In", n_experiments, "experiments, the maximum heads was:", max_heads, "\n")
```

```
## In 1000 experiments, the maximum heads was: 15
```

``` r
cat("Probability of getting at least", max_heads, "heads by chance:",
    mean(results &gt;= max_heads), "\n")
```

```
## Probability of getting at least 15 heads by chance: 0.002
```

---
###The Multiple Comparisons Problem

Every significance test we carry out is [a chance to falsely conclude that a relationship is meaningful due to chance](https://xkcd.com/882/).

---

If we aren't careful, most times that we pay attention to multiple significance tests related to the same data and the same problem, we are at risk of finding false precision in our results.

---
###Why This Happens

Remember that we noted 0.05 as a probability level in significance tests that leads to excitement. This is often written as `\(\alpha = 0.05\)`. If our significance test has been done well, meeting the assumptions we talked about last time, the probability of getting a result of 0.05 or less on a single significance test that corresponds with a population regression slope that is actually 0 is 0.05.

So this won't happen very often, and if we have a single result below this level, we can treat it seriously with meaningful confidence.

---

However, suppose that we carry out 20 independent significance tests --- including 20 unrelated independent variables in our regression, for example. Now, we can think of each significance test as being like a coin flip in our example above. The chance of finding at least one heads --- at least one value randomly below 0.05 even though the population regression slope is zero --- is going to be much higher than 0.05:


``` r
1 - (0.95)^20
```

```
## [1] 0.6415141
```

---

&lt;img src="MultipleComparisons_files/figure-html/unnamed-chunk-6-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

We have two main measures of failure when carrying out multiple comparisons:

1. The *family-wide error rate*, which is the probability of incorrectly rejecting even one null hypothesis.

2. The *false discovery rate*, which is the expected proportion of false discoveries (i.e., findings for which we reject the null hypothesis) among all discoveries.

---
###Family-Wide Error Rate

`$$\text{FWER} = \text{Pr}(\text{at least one false positive})$$`
Intuition: “What’s the chance that any of my ‘significant’ findings is just noise?”

Use case: When you need strong control over false positives across all tests (e.g., confirmatory studies, many RCTs).

---
###False-Discovery Rate

`$$\text{FDR} = \text{E}(\frac{\text{False Positives}}{\text{All Positives}})$$`

---
###Bonferroni

The first and simplest way to correct the family-wide error rate is the Bonferroni correction: just divide `\(\alpha\)` by the number of tests you're carrying out.

Suppose our target significance level is `\(\alpha\)` and we're carrying out `\(m\)` total tests. Then the Bonferroni correction is to set our actual target significance level for each individual test at `\(\frac{\alpha}{m}\)` but interpret the results as still just implying a family-wide error rate of not more than `\(\alpha\)`.

---

To see why, consider our example of 20 independent tests from earlier. We want `\(\alpha = 0.05\)`, so our Bonferroni-corrected target will be `\(\frac{0.05}{20} = 0.0025\)`.


``` r
1 - (1 - 0.0025)^20
```

```
## [1] 0.04883012
```

---

If our tests are not independent, the Bonferroni correction will typically be too conservative.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
