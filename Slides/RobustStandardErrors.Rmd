---
title: "17: Robust Standard Errors."
subtitle: "Linear Models"
author: "<large>Jaye Seawright</large>"
institute: "<small>Northwestern Political Science</small>" 
date: "March 4, 2026"
output: 
  xaringan::moon_reader:
    css: xaringan-themer.css
  
---
class: center, middle


```{css, echo=FALSE}
pre {
  max-height: 400px;
  overflow-y: auto;
}

pre[class] {
  max-height: 200px;
}
```

```{r, load_refs, include=FALSE, cache=FALSE}
# Initializes
library(RefManageR)

library(ggplot2)
library(dplyr)
library(readr)
library(nlme)
library(jtools)
library(mice)
library(knitr)
library(modelsummary)
library(kableExtra)
library(stringr)
library(ggplot2)
library(dplyr)
library(sandwich)
library(lmtest)
library(clubSandwich)
library(fixest)
library(modelsummary)
library(AER)

BibOptions(check.entries = FALSE,
           bib.style = "authoryear", # Bibliography style
           max.names = 3, # Max author names displayed in bibliography
           sorting = "nyt", #Name, year, title sorting
           cite.style = "authoryear", # citation style
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)

library(rqog)
qogts <- read_qog(which_data="standard", data_type = "time-series")

```
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer,MnSymbol)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono"),
  text_font_size = "1.6rem"
)

knitr::opts_chunk$set(warning = FALSE, message = FALSE)

```

### The Problem: Why We Need Robust SEs

---

### **Classical OLS Assumption:**
$$
\text{Var}(\epsilon_i | X) = \sigma^2
$$
Constant variance (homoskedasticity)

---

### **Reality:**
- Some groups are more predictable than others
- Richer districts have more variation in outcomes
- Survey error varies by respondent type
- Time-series volatility changes over time

**This is heteroskedasticity**

---

```{r hetero-example, echo=FALSE, fig.height=4.5}
set.seed(123)
n <- 200
x <- runif(n, 10, 100)
# Heteroskedastic errors: variance increases with x
sigma <- 0.1 * x
y <- 2 + 0.5*x + rnorm(n, 0, sigma)

par(mfrow = c(1, 2))
plot(x, y, pch = 19, col = rgb(0, 0, 0, 0.5),
     main = "Heteroskedastic Data",
     xlab = "X (e.g., District Income)",
     ylab = "Y (e.g., Vote Share)")
abline(lm(y ~ x), col = "red", lwd = 2)

plot(fitted(lm(y ~ x)), resid(lm(y ~ x)),
     pch = 19, col = rgb(0, 0, 0, 0.5),
     main = "Residual Plot (Funnel Shape)",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

---

**Consequence:** Standard errors are wrong → confidence intervals wrong → p-values wrong

---
### **What Happens?**

1. **OLS coefficients remain unbiased**
2. **But variance estimates are biased:**
   - Can be too small (common) → false significance
   - Can be too large → missed findings

---
### **Examples from Political Science:**
- **Wealth and voting:** Richer districts show different patterns of variation in turnout than poor districts (e.g., Avery 2015)
- **Media effects:** Highly attentive individuals are differently predictable compared to less attentive people (e.g., Walgrave et al., 2017)
- **Policy adoption:** Effects may differ across space and time (e.g. Matisoff 2008)
- **Conflict studies:** Actors have different distributions of information, motivations, and resources (e.g., Bas 2012)

---

```{r consequences, echo=FALSE}
# Simulate the problem
set.seed(123)
n <- 100
reps <- 1000

# Store results
results <- data.frame(
  homoskedastic = numeric(reps),
  heteroskedastic = numeric(reps)
)

for(i in 1:reps) {
  x <- rnorm(n)
  # Homoskedastic case
  y_homo <- 0.5*x + rnorm(n, 0, 1)
  fit_homo <- lm(y_homo ~ x)
  
  # Heteroskedastic case
  y_hetero <- 0.5*x + rnorm(n, 0, abs(x) + 0.5)
  fit_hetero <- lm(y_hetero ~ x)
  
  results$homoskedastic[i] <- summary(fit_homo)$coefficients[2, 2]
  results$heteroskedastic[i] <- summary(fit_hetero)$coefficients[2, 2]
}

cat("Average SE (homoskedastic):", round(mean(results$homoskedastic), 3), "\n")
cat("Average SE (heteroskedastic):", round(mean(results$heteroskedastic), 3), "\n")
cat("\nTypical underestimation:", 
    round((mean(results$homoskedastic) - mean(results$heteroskedastic)) / 
            mean(results$heteroskedastic) * 100, 1), "%")
```

---

```{r consequence-plot, echo=FALSE, fig.height=8}
# Plot comparison
library(tidyr)
results_long <- pivot_longer(results, everything())

ggplot(results_long, aes(x = value, fill = name)) +
  geom_density(alpha = 0.5) +
  labs(x = "Estimated Standard Error",
       y = "Density",
       title = "SE Estimates Under Different Conditions") +
  scale_fill_manual(values = c("blue", "red"),
                    labels = c("Homoskedastic", "Heteroskedastic")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

---
### **Huber-White (Sandwich) Estimator**
Developed by Huber (1967) and White (1980)

**Regular OLS variance:**
$$
\text{Var}(\hat{\beta}) = (X'X)^{-1} \sigma^2
$$

**Robust (HC) variance:**
$$
\text{Var}(\hat{\beta}) = (X'X)^{-1} X' \Omega X (X'X)^{-1}
$$
where $\Omega = \text{diag}(e_i^2)$ for HC0

---
### **Intuition:**
- Weight observations by their residual size
- Bigger residuals → more uncertainty in that region
- Adapts to heteroskedasticity

---
### **Flavors of HC Estimators**
```{r hc-types, echo=FALSE}
library(gt)

hc_data <- data.frame(
  Type = c("HC0", "HC1", "HC2", "HC3", "HC4"),
  Formula = c("\\(e_i^2\\)", 
              "\\(\\frac{n}{n-k}e_i^2\\)", 
              "\\(\\frac{e_i^2}{1-h_{ii}}\\)", 
              "\\(\\frac{e_i^2}{(1-h_{ii})^2}\\)",
              "\\(\\frac{e_i^2}{(1-h_{ii})^{\\delta_i}}\\)"),
  Description = c("Basic White", 
                  "Incomplete small sample correction",
                  "Leverage adjustment",
                  "Better small sample correction",
                  "Designed for outliers")
)

gt(hc_data) %>%
  cols_label(
    Type = "Type",
    Formula = "Weight",
    Description = "Description"
  ) %>%
  tab_header(title = "Common HC Estimators") %>%
  tab_style(
    style = cell_text(size = px(16)),
    locations = cells_body()
  )
```

---
### **Social Science Practice:**
- **HC1**, **HC2**, and **HC3** converge for larger samples (above about 500)
- **HC3** performs best for smaller samples and works well down to samples of about 25 
- **Stata default:** HC1, in spite of its limitations
- **R common practice:** HC3, which is probably the best choice (Long and Ervin 2000)
]

---
### Implementation in R: The `sandwich` Package

---
### **Basic Implementation**

```{r r-implementation, eval=TRUE}
library(sandwich)
library(lmtest)

la_electoral <- read.csv("https://raw.githubusercontent.com/jnseawright/ps405/refs/heads/main/Data/laelectoral.csv")

# After fitting model
# Fit regular model
model <- lm(voteswoninc ~ gdpgrowthlag + votesinclag, data = la_electoral)
```

---

```{r r-implementation-15, eval=TRUE}
# Regular SEs
summary(model)
```
---
  
```{r r-implementation-2, eval=TRUE}
# Robust SEs (HC3 default)
coeftest(model, vcov = vcovHC)
```

---

```{r r-implementation-3, eval=TRUE}
# Different HC types
coeftest(model, vcov = vcovHC(model, type = "HC0"))
```

---
  
```{r r-implementation-4, eval=TRUE}
coeftest(model, vcov = vcovHC(model, type = "HC1"))
```

---
  
```{r r-implementation-5, eval=TRUE}
coeftest(model, vcov = vcovHC(model, type = "HC3"))
```

---
### **Using `modelsummary`**
```{r modelsummary-ex, eval=TRUE}
library(modelsummary)

ms.display <- modelsummary(
  list("OLS" = model, 
       "Robust SE" = model),
  vcov = list("iid", "HC3"),
  stars = TRUE, output = "kableExtra"
)
```

---

```{r modelsummary-ex-2, eval=TRUE, echo=FALSE}
ms.display
```

---
### **Complete Example**
```{r full-r-example, eval=TRUE}
# Load data
data("CASchools", package = "AER")
# Simulate heteroskedasticity
CASchools$testscore <- 
  CASchools$read + CASchools$math + 
  rnorm(nrow(CASchools), 0, 
        abs(CASchools$income)/1000)

# Fit model
model <- lm(testscore ~ income + english, 
            data = CASchools)

# Compare SEs
library(modelsummary)
models <- list(
  "Regular" = model,
  "HC0" = model,
  "HC1" = model,
  "HC2" = model,
  "HC3" = model
)

caschools.msummary <- msummary(models,
         vcov = c("iid", "HC0", "HC1", "HC2", "HC3"),
         fmt = 2,
         stars = TRUE,
         output = "kableExtra") %>%
  kable_styling(font_size = 10)
```

---

```{r caschools, eval=TRUE, echo=FALSE}
caschools.msummary
```

---

### **Key Insight:**
Robust SEs often (but not always) increase standard errors, making findings **less significant**. This is more honest given real-world data.

---
# Clustered Standard Errors

### **When Observations Are Not Independent**
Common in political science:
1. **Panel data:** Multiple observations per country
2. **Survey data:** Respondents within states
3. **Experimental data:** Treatments assigned to groups
4. **Spatial data:** Nearby locations correlated

---
### **The Problem:**
Within-cluster correlation → underestimation of variance

**Example:** If all respondents in Texas share unobserved factors, we don't have 100 independent observations, but 1 cluster of 100 correlated observations

---
### **Clustering Formula**
$$
\text{Var}(\hat{\beta}) = (X'X)^{-1} \left( \sum_{c=1}^C X_c' e_c e_c' X_c \right) (X'X)^{-1}
$$
where $c$ indexes clusters

---
### **Implementation in R:**
```{r cluster-r, eval=FALSE}
# Using sandwich and lmtest
coeftest(model, 
         vcov = vcovCL(model, 
                       cluster = ~state))

# Using fixest package (fast!)
library(fixest)
model_fe <- feols(vote_share ~ income + education,
                  data = election_data,
                  cluster = ~state)

# Using clubSandwich for multi-way clustering
library(clubSandwich)
coef_test(model, 
          vcov = "CR2", 
          cluster = election_data$state)
```

---
# Multi-Way Clustering

### **Complex Dependencies**
Sometimes observations cluster in multiple ways:
1. **States and years** (Cameron, Gelbach & Miller, 2011)
2. **Individual and time** (two-way FE)
3. **Industry and region**

---

### **R Implementation:**
```{r multiway-cluster, eval=FALSE}
# Using clubSandwich
vcov_multi <- vcovCR(model,
                     type = "CR2",
                     cluster = election_data[, c("state", "year")])

coeftest(model, vcov = vcov_multi)

# Using lfe (older but works)
library(lfe)
model_multi <- felm(vote_share ~ income + education |
                      state + year,
                    data = election_data)
summary(model_multi, robust = TRUE)
```

---
### **When to Cluster?**

```{r clustering-rules, echo=FALSE}
rules <- data.frame(
  Data_Structure = c("Cross-section", 
                     "Panel (country-year)",
                     "Survey (individuals in states)",
                     "Experimental (treatments in groups)",
                     "Time series",
                     "Network/spatial"),
  Clustering = c("No clustering or heteroskedasticity-robust",
                 "Cluster by country",
                 "Cluster by state (or primary sampling unit)",
                 "Cluster by treatment group",
                 "Newey-West (HAC) errors",
                 "Spatial HAC or Conley errors"),
  R_Package = c("sandwich", "plm/sandwich", "survey/sandwich", 
                "clubSandwich", "sandwich", "spatialreg")
)

knitr::kable(rules, 
             caption = "Clustering Guidelines for Political Data") %>%
  kableExtra::kable_styling(font_size = 10)
```

---

**Rule of thumb:** Cluster at the level of treatment assignment or at the level where you suspect correlation.

---
# Limitations and When NOT to Use Robust SEs

### **1. Very Small Samples**
- HC estimators biased in tiny samples
- N < 25: Consider alternatives
- Better: Use exact methods or permutation tests

### **2. Weighted Data**
- Survey weights already account for heterogeneity
- Don't double-adjust

---

### **3. Nonlinear Models**
- GLMs (logit, probit) have different variance structure

### **4. Model Misspecification**
Robust SEs don't fix:
- Omitted variable bias
- Functional form misspecification
- Measurement error
- Simultaneity

---

### **5. Efficiency Loss**
- Robust SEs are less efficient when homoskedasticity holds

Use robust SEs unless you have strong reason not to

---

![Robust1](Images/Xu1.png)

---

![Robust2](Images/Xu2.png)