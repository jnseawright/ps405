<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>13: Statistical Power.</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jaye Seawright" />
    <meta name="date" content="2026-02-18" />
    <script src="StatisticalPower_files/header-attrs-2.30/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 13: Statistical Power.
]
.subtitle[
## Linear Models
]
.author[
### <large>Jaye Seawright</large>
]
.institute[
### <small>Northwestern Political Science</small>
]
.date[
### Feb.Â 18, 2026
]

---

class: center, middle

&lt;style type="text/css"&gt;
pre {
  max-height: 400px;
  overflow-y: auto;
}

pre[class] {
  max-height: 200px;
}
&lt;/style&gt;




If we want a good estimate of `\(\hat \beta_1\)`, with a small standard error and a good chance of statistically significant results if the population value of `\(\beta_1\)` is not close to zero, how large should we make our sample?

---
###Statistical Power

*Statistical power* is the probability, before data are gathered, that a particular test will pass a threshold at some predetermined significance level (typically a p-value below 0.05), given some assumed population parameter.

`$$\small\text{Power} = \text{Pr}_{\text{Prior}} (\text{Reject } H_{0} | \text{a specific version of }H_{1} \text{ is true})$$`

---
###Problems with Low Power

1. *High Rate of False Negatives*: You will frequently fail to reject the null hypothesis (Type II errors), even when a meaningful effect exists. This leads to missed discoveries and can bias the literature through publication bias.

2. *"Significant" Results are Often Misleading*: When you do find a statistically significant result (p &lt; 0.05) in a low-power study, the estimated effect is likely exaggerated or could be a false positive. The low signal-to-noise ratio (high standard errors) means that passing the high statistical threshold often requires a large, luck-driven estimate.

---

&lt;img src="StatisticalPower_files/figure-html/unnamed-chunk-2-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="StatisticalPower_files/figure-html/unnamed-chunk-3-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="StatisticalPower_files/figure-html/unnamed-chunk-4-1.png" width="80%" style="display: block; margin: auto;" /&gt;


``` r
# Load required libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(patchwork)  # For combining plots
library(scales)     # For better scale formatting

# Set seed for reproducibility
set.seed(2024)

# Simulation parameters
n_simulations &lt;- 10000   # Number of studies to simulate
true_effect_size &lt;- 0.2  # True population effect (e.g., correlation coefficient)
alpha &lt;- 0.05           # Significance level

# Create a function to simulate one study
simulate_study &lt;- function(n, true_effect, alpha_level = 0.05) {
  # Generate data for two groups (treatment and control)
  # Using a simple normal model: Y = treatment_effect + error
  control_group &lt;- rnorm(n/2, mean = 0, sd = 1)
  treatment_group &lt;- rnorm(n/2, mean = true_effect, sd = 1)
  
  # Run a t-test
  test_result &lt;- t.test(treatment_group, control_group, 
                        var.equal = TRUE)
  
  # Calculate observed effect size (standardized bivariate coefficient for treatment v. control)
  pooled_sd &lt;- sqrt(((n/2 - 1) * var(control_group) + 
                      (n/2 - 1) * var(treatment_group)) / (n - 2))
  observed_d &lt;- (mean(treatment_group) - mean(control_group)) / pooled_sd
  
  # Return results
  return(data.frame(
    n = n,
    p_value = test_result$p.value,
    observed_effect = observed_d,
    significant = test_result$p.value &lt; alpha_level,
    ci_lower = test_result$conf.int[1] / pooled_sd,
    ci_upper = test_result$conf.int[2] / pooled_sd
  ))
}

# Run simulation for different sample sizes (low vs high power)
sample_sizes &lt;- c(50, 200, 800)  # Small, medium, large N
results &lt;- data.frame()

for (n in sample_sizes) {
  for (i in 1:n_simulations) {
    study_result &lt;- simulate_study(n, true_effect_size, alpha)
    results &lt;- rbind(results, study_result)
  }
}

# Calculate power for each sample size
power_calc &lt;- results %&gt;%
  group_by(n) %&gt;%
  summarize(
    power = mean(significant),
    mean_observed = mean(observed_effect),
    mean_observed_sig = mean(observed_effect[significant]),
    prop_sig = mean(significant),
    n_studies = n()
  ) %&gt;%
  mutate(
    power_theoretical = round(power * 100, 1),
    label = paste0("N = ", n, "\nPower = ", power_theoretical, "%")
  )

# Create Plot 1: Distribution of observed effects (all studies)
p1 &lt;- ggplot(results, aes(x = observed_effect, fill = factor(n))) +
  geom_density(alpha = 0.6, adjust = 1.5) +
  geom_vline(xintercept = true_effect_size, 
             linetype = "dashed", color = "red", size = 1) +
  facet_wrap(~ n, ncol = 1, 
             labeller = labeller(n = function(x) paste0("N = ", x))) +
  labs(
    title = "Distribution of Estimated Coefficients",
    subtitle = paste0("Population Coefficient (red line) = ", true_effect_size),
    x = "Estimated Coefficient",
    y = "Density",
    caption = "All simulated studies (significant and non-significant)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    plot.title = element_text(face = "bold", size = 14),
    strip.text = element_text(face = "bold", size = 11)
  ) +
  scale_fill_brewer(palette = "Set2")

# Create Plot 2: "Winner's Curse" - Only significant results
p2 &lt;- ggplot(results %&gt;% filter(significant == TRUE), 
             aes(x = observed_effect, fill = factor(n))) +
  geom_density(alpha = 0.6, adjust = 1.5) +
  geom_vline(xintercept = true_effect_size, 
             linetype = "dashed", color = "red", size = 1) +
  facet_wrap(~ n, ncol = 1,
             labeller = labeller(n = function(x) {
               power_val &lt;- power_calc$power_theoretical[power_calc$n == as.numeric(x)]
               paste0("N = ", x, " (Power = ", power_val, "%)")
             })) +
  labs(
    title = '"Winner\'s Curse": Distribution of Statistically Significant Estimates',
    subtitle = paste0("Population Coefficient (red line) = ", true_effect_size),
    x = "Estimated Coefficient",
    y = "Density",
    caption = "Only studies with p &lt; 0.05 (the 'winners')"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "none",
    plot.title = element_text(face = "bold", size = 14),
    strip.text = element_text(face = "bold", size = 10)
  ) +
  scale_fill_brewer(palette = "Set2") +
  coord_cartesian(xlim = c(-0.5, 1.5))  # Consistent x-axis

# Create Plot 3: Distortions in effect estimates (summary)
summary_data &lt;- results %&gt;%
  group_by(n, significant) %&gt;%
  summarize(
    mean_effect = mean(observed_effect),
    median_effect = median(observed_effect),
    sd_effect = sd(observed_effect),
    n_studies = n(),
    .groups = 'drop'
  ) %&gt;%
  mutate(
    significance = ifelse(significant, "Significant (p &lt; 0.05)", "Non-significant"),
    bias = mean_effect - true_effect_size
  )

p3 &lt;- ggplot(summary_data, 
             aes(x = factor(n), y = mean_effect, fill = significance)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), 
           width = 0.8, alpha = 0.8) +
  geom_hline(yintercept = true_effect_size, 
             linetype = "dashed", color = "red", size = 1) +
  geom_errorbar(aes(ymin = mean_effect - 1.96 * sd_effect / sqrt(n_studies),
                    ymax = mean_effect + 1.96 * sd_effect / sqrt(n_studies)),
                position = position_dodge(width = 0.9),
                width = 0.3) +
  labs(
    title = "Distortions in Coefficient Estimates by Significance and Sample Size",
    subtitle = paste0("Population Coefficient (red line) = ", true_effect_size),
    x = "Sample Size (N)",
    y = "Mean Estimated Coefficient",
    fill = "Statistical Significance"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    axis.text.x = element_text(size = 11)
  ) +
  scale_fill_manual(values = c("Non-significant" = "#999999", 
                               "Significant (p &lt; 0.05)" = "#E69F00"))

# Create Plot 4: Probability of overestimation
overestimation_data &lt;- results %&gt;%
  group_by(n) %&gt;%
  summarize(
    prop_overestimate_sig = mean(observed_effect[significant] &gt; true_effect_size),
    prop_severe_overestimate_sig = mean(observed_effect[significant] &gt; true_effect_size * 1.5),
    n_sig = sum(significant)) %&gt;%
  pivot_longer(cols = starts_with("prop"), 
               names_to = "metric", 
               values_to = "probability") %&gt;%
  mutate(
    metric = case_when(
      metric == "prop_overestimate_sig" ~ "Significant studies overestimate population coefficient",
      metric == "prop_severe_overestimate_sig" ~ "Significant studies &gt;1.5x population coefficient"),
    metric = factor(metric, levels = c(
                                       "Significant studies overestimate population coefficient",
                                       "Significant studies &gt;1.5x population coefficient")))

p4 &lt;- ggplot(overestimation_data, 
             aes(x = factor(n), y = probability, fill = metric)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), 
           width = 0.8, alpha = 0.8) +
  scale_y_continuous(labels = percent_format()) +
  labs(
    title = "Probability of Overestimation",
    subtitle = "The 'Winner's Curse' becomes more severe with lower power",
    x = "Sample Size (N)",
    y = "Probability",
    fill = "Type of Overestimation"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    legend.text = element_text(size = 9)
  ) +
  scale_fill_brewer(palette = "Set2") +
  geom_text(aes(label = paste0(round(probability * 100, 1), "%")),
            position = position_dodge(width = 0.9),
            vjust = -0.5, size = 3.5)
```

---

&lt;img src="StatisticalPower_files/figure-html/unnamed-chunk-6-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="StatisticalPower_files/figure-html/unnamed-chunk-7-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="StatisticalPower_files/figure-html/unnamed-chunk-8-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="StatisticalPower_files/figure-html/unnamed-chunk-9-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

When designing a study:

1. *Increase signal strength first*. This can involve designing experimental treatments to be as strong as possible, choosing participants to be as susceptible to the cause of interest as you can, etc. Increasing the distance between the population parameter and zero quickly increases your power and reduces the winner's curse.

---

When designing a study:

2. *Minimize noise through design.* Randomness isn't always a given, any more than effect size is. You can use clever sampling strategies, improve measurement, or compare before and after within the same subjects to reduce overall variation. Reducing variation is another great way to increase power.

---

When designing a study:

3. *Increase sample size.* When other choices are unavailable, increasing sample size is an expensive but still valid way to avoid the problems of low power and resulting low credibility.

---
### Reading Published Research

* *Expect overestimation.* Published effect sizes tend to be inflated due to the winner's curse that we've discussed, given that significant results are conditionally biased upward.

* *Why?* The publishing process selects for statistical significance. Conditional on `\(p &lt; 0.05\)`, estimates tend to exceed the true effect, especially when power is low.

---

There are published formulae for several different kinds of power calculations. Or you can use R.

---


``` r
library(pwrss)

power.t.regression(beta = 0.60,
                   sd.outcome = 12,
                   sd.predictor = 4,
                   k.total = 12,
                   r.squared = 0.30,
                   power = .80,
                   alpha = 0.05,
                   alternative = "two.sided")
```

```
## +--------------------------------------------------+
## |             SAMPLE SIZE CALCULATION              |
## +--------------------------------------------------+
## 
## Linear Regression Coefficient (T-Test)
## 
## ---------------------------------------------------
## Hypotheses
## ---------------------------------------------------
##   H0 (Null Claim) : beta - null.beta = 0 
##   H1 (Alt. Claim) : beta - null.beta != 0 
## 
## ---------------------------------------------------
## Results
## ---------------------------------------------------
##   Sample Size            = 140  &lt;&lt;
##   Type 1 Error (alpha)   = 0.050
##   Type 2 Error           = 0.198
##   Statistical Power      = 0.802
```

---


``` r
library(pwrss)

power.t.regression(beta = 0.10,
                   sd.outcome = 12,
                   sd.predictor = 4,
                   k.total = 12,
                   r.squared = 0.30,
                   power = .80,
                   alpha = 0.05,
                   alternative = "two.sided")
```

```
## +--------------------------------------------------+
## |             SAMPLE SIZE CALCULATION              |
## +--------------------------------------------------+
## 
## Linear Regression Coefficient (T-Test)
## 
## ---------------------------------------------------
## Hypotheses
## ---------------------------------------------------
##   H0 (Null Claim) : beta - null.beta = 0 
##   H1 (Alt. Claim) : beta - null.beta != 0 
## 
## ---------------------------------------------------
## Results
## ---------------------------------------------------
##   Sample Size            = 4947  &lt;&lt;
##   Type 1 Error (alpha)   = 0.050
##   Type 2 Error           = 0.200
##   Statistical Power      = 0.8
```

---


``` r
library(pwrss)

power.t.regression(beta = 0.60,
                   sd.outcome = 60,
                   sd.predictor = 4,
                   k.total = 12,
                   r.squared = 0.30,
                   power = .80,
                   alpha = 0.05,
                   alternative = "two.sided")
```

```
## +--------------------------------------------------+
## |             SAMPLE SIZE CALCULATION              |
## +--------------------------------------------------+
## 
## Linear Regression Coefficient (T-Test)
## 
## ---------------------------------------------------
## Hypotheses
## ---------------------------------------------------
##   H0 (Null Claim) : beta - null.beta = 0 
##   H1 (Alt. Claim) : beta - null.beta != 0 
## 
## ---------------------------------------------------
## Results
## ---------------------------------------------------
##   Sample Size            = 3436  &lt;&lt;
##   Type 1 Error (alpha)   = 0.050
##   Type 2 Error           = 0.200
##   Statistical Power      = 0.8
```


---


``` r
library(pwrss)

power.t.regression(beta = 0.60,
                   sd.outcome = 12,
                   sd.predictor = 4,
                   k.total = 12,
                   r.squared = 0.10,
                   power = .80,
                   alpha = 0.05,
                   alternative = "two.sided")
```

```
## +--------------------------------------------------+
## |             SAMPLE SIZE CALCULATION              |
## +--------------------------------------------------+
## 
## Linear Regression Coefficient (T-Test)
## 
## ---------------------------------------------------
## Hypotheses
## ---------------------------------------------------
##   H0 (Null Claim) : beta - null.beta = 0 
##   H1 (Alt. Claim) : beta - null.beta != 0 
## 
## ---------------------------------------------------
## Results
## ---------------------------------------------------
##   Sample Size            = 179  &lt;&lt;
##   Type 1 Error (alpha)   = 0.050
##   Type 2 Error           = 0.199
##   Statistical Power      = 0.801
```

---

How useful is a power calculation after the study has been carried out?
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
